import torch
#torch.autograd.set_detect_anomaly(True)
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR  # å¯¼å…¥ CosineAnnealingLR
import os

def check_tensors(step_name, tensor_dict):
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥å­—å…¸ä¸­çš„å¼ é‡æ˜¯å¦å­˜åœ¨ NaN æˆ– Infã€‚"""
    for name, tensor in tensor_dict.items():
        if tensor is not None and torch.isinf(tensor).any():
            print(f"!!!!!!!!!!!!!! åœ¨ {step_name} å¤„ï¼Œå¼ é‡ '{name}' ä¸­æ£€æµ‹åˆ° Inf !!!!!!!!!!")
            # åœ¨æ£€æµ‹åˆ°é—®é¢˜çš„ç¬¬ä¸€ä¸ªåœ°æ–¹å°±å¼ºåˆ¶é€€å‡ºï¼Œä»¥ä¾¿æ£€æŸ¥
            raise RuntimeError(f"Inf detected in tensor '{name}' at step '{step_name}'")
        if tensor is not None and torch.isnan(tensor).any():
            print(f"!!!!!!!!!!!!!! åœ¨ {step_name} å¤„ï¼Œå¼ é‡ '{name}' ä¸­æ£€æµ‹åˆ° NaN !!!!!!!!!!")
            raise RuntimeError(f"NaN detected in tensor '{name}' at step '{step_name}'")

# ==============================================================================
# 1. è¾…åŠ©å‡½æ•° (Helper Functions)
# ==============================================================================

# def scale_to_unit_sphere(pos: torch.Tensor, batch_map: torch.Tensor) -> torch.Tensor:
#     """
#     å°†æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾çš„åæ ‡ç‹¬ç«‹åœ°ç¼©æ”¾åˆ°å•ä½çƒå†…ã€‚
    
#     Args:
#         pos (torch.Tensor): æ‰¹æ¬¡ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åæ ‡å¼ é‡, shape [N, 3]ã€‚
#         batch_map (torch.Tensor): å°†æ¯ä¸ªèŠ‚ç‚¹æ˜ å°„åˆ°å…¶æ‰€å±å›¾çš„å‘é‡, shape [N]ã€‚
    
#     Returns:
#         torch.Tensor: ç¼©æ”¾åçš„åæ ‡ã€‚
#     """
#     # å‡è®¾æˆ‘ä»¬çš„è¾“å…¥ pos æ˜¯ä¸€ä¸ª [N, 3] çš„å¼ é‡ï¼Œä»£è¡¨æ‰¹æ¬¡ä¸­æ‰€æœ‰ N ä¸ªåŸå­çš„åæ ‡ï¼›
#     # batch_map æ˜¯ä¸€ä¸ª [N] çš„å¼ é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªåŸå­å±äºå“ªä¸ªåˆ†å­å›¾ï¼ˆ0, 0, 0, 1, 1, ...ï¼‰ã€‚
#     # PyG çš„ scatter å‡½æ•°å¯ä»¥é«˜æ•ˆåœ°æŒ‰ç»„æ±‚å’Œ/æ±‚å‡å€¼
#     from torch_geometric.utils import scatter
    
#     # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åˆ°å…¶è´¨å¿ƒçš„è·ç¦»
#     # è¾“å…¥ pos çš„ç»´åº¦æ˜¯ [N, 3]
#     # e.g. å¯¹äºç¬¬ 0 è¡Œ [x_0, y_0, z_0]ï¼Œå®ƒè®¡ç®— sqrt(x_0Â² + y_0Â² + z_0Â²)
#     # torch.linalg.norm æ²¿ç€ dim=1 è¿›è¡Œæ“ä½œï¼Œè¿™ä¸ªç»´åº¦åœ¨è®¡ç®—åä¼šæ¶ˆå¤±
#     # è¾“å‡º distances çš„ç»´åº¦æ˜¯ [N]
#     distances = torch.linalg.norm(pos, dim=1)
    
#     # æŒ‰å›¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå›¾ä¸­çš„æœ€å¤§è·ç¦»
#     max_distances = scatter(distances, batch_map, dim=0, reduce='max') # shape: [num_graphs]
    
#     # è®¡ç®—æ¯ä¸ªå›¾çš„ç¼©æ”¾å› å­ï¼ŒåŠ ä¸Šä¸€ä¸ªå°çš„ epsilon é˜²æ­¢é™¤ä»¥é›¶
#     scale_factors = max_distances[batch_map].unsqueeze(1) + 1e-8

    
#     # ç¼©æ”¾åæ ‡
#     return pos / scale_factors


def noise_discrete_features(
    features_0: torch.Tensor,
    Q_bar: torch.Tensor,
    t_per_item: torch.Tensor
) -> torch.Tensor:
    """
    å¯¹ one-hot ç¼–ç çš„ç¦»æ•£ç‰¹å¾ï¼ˆå¦‚åŸå­ç±»å‹ã€è¾¹ç±»å‹ï¼‰è¿›è¡ŒåŠ å™ªã€‚

    Args:
        features_0 (torch.Tensor): å¹²å‡€çš„ one-hot ç‰¹å¾, shape [M, K] (Mä¸ªé¡¹ç›®, Kä¸ªç±»åˆ«)ã€‚
        Q_bar (torch.Tensor):      è½¬ç§»çŸ©é˜µé›†åˆ, shape [T, K, K] (Tä¸ªæ—¶é—´æ­¥)ã€‚
        t_per_item (torch.Tensor): æ¯ä¸ªé¡¹ç›®å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]ã€‚

    Returns:
        torch.Tensor: åŠ å™ªåçš„ one-hot ç‰¹å¾ã€‚
    """
    # 1. æ ¹æ®æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ tï¼Œä» Q_bar ä¸­é€‰å‡ºå¯¹åº”çš„è½¬ç§»çŸ©é˜µ
    # Q_bar_t çš„ shape ä¸º [M, K, K]
    Q_bar_t = Q_bar[t_per_item] 
    
    # 2. è®¡ç®—åŠ å™ªåçš„æ¦‚ç‡åˆ†å¸ƒ
    # features_0.unsqueeze(1) -> [M, 1, K]   åœ¨ features_0 çš„ç¬¬1ä¸ªç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
    # Q_bar_t                 -> [M, K, K]
    # prob_t                  -> [M, 1, K]
    # æ‰§è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
    prob_t = torch.bmm(features_0.unsqueeze(1), Q_bar_t).squeeze(1) # shape: [M, K]
    
    # 3. æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°æ–°çš„ç±»åˆ«ç´¢å¼•
    # torch.multinomial è¦æ±‚è¾“å…¥æ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬è¿™é‡Œå·²ç»æ˜¯æ¦‚ç‡äº†
    # multinomial ä¼šæŠŠæ¯ä¸€è¡Œéƒ½çœ‹ä½œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„â€œéª°å­â€çš„æ¦‚ç‡è®¾ç½®
    # num_samples=1 æŒ‡æ¯è¡Œé‡‡æ ·ä¸€æ¬¡ï¼Œè¿™æ—¶è¾“å‡ºç»´åº¦ä¸º[M, 1]
    # .squeeze(-1) ä½œç”¨ä¸ºç§»é™¤æœ€åä¸€ä¸ªç»´åº¦ï¼ˆdim=-1ï¼‰ä¸Šå¤§å°ä¸º1çš„ç»´åº¦
    sampled_indices = torch.multinomial(prob_t, num_samples=1).squeeze(-1) # shape: [M]
    
    # 4. å°†é‡‡æ ·å‡ºçš„ç´¢å¼•è½¬æ¢å› one-hot ç¼–ç 
    num_classes = features_0.shape[1]  # è·å–ç±»åˆ«çš„æ€»æ•°
    # å°†æ•´æ•°ç±»åˆ«ç´¢å¼•è½¬æ¢æˆ One-Hot ç¼–ç å‘é‡
    features_t = torch.nn.functional.one_hot(sampled_indices, num_classes=num_classes).float()
    
    return features_t

# ==============================================================================
# 2. æŸå¤±å‡½æ•°æ¡†æ¶ (Loss Function Skeletons)
# ==============================================================================

# 2.1 åŸå­ç±»å‹æŸå¤±
# æ¨èä½¿ç”¨çš„ã€æ›´ç®€æ´çš„æŸå¤±å‡½æ•°
def calculate_atom_type_loss(
    pred_logits: torch.Tensor,   # æ¨¡å‹å¯¹ x0 çš„é¢„æµ‹ logits, shape [M, C]
    true_x0_indices: torch.Tensor, # çœŸå®çš„ x0 ç±»åˆ«ç´¢å¼•, shape [M]
    lambda_aux: float = 0.001,     # D3PMè®ºæ–‡å»ºè®®çš„å°å€¼
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆåŸå­ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_x0_indices: çœŸå®çš„ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """

    # --- [æ–°] è°ƒè¯•æ£€æŸ¥ ---
    if torch.isnan(pred_logits).any() or torch.isinf(pred_logits).any():
        print("!!! è­¦å‘Š: åœ¨è®¡ç®—æŸå¤±ä¹‹å‰ï¼Œåœ¨ pred_logits ä¸­æ£€æµ‹åˆ° NaN æˆ– Inf !!!")
        # å¯é€‰ï¼šä¿å­˜æœ‰é—®é¢˜çš„å¼ é‡ï¼Œä»¥ä¾¿åç»­åˆ†æ
        # torch.save(pred_logits, "problematic_logits.pt")
        
    # åŒæ—¶æ£€æŸ¥ç›®æ ‡ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
    num_classes = pred_logits.shape[-1]
    if true_x0_indices.max() >= num_classes or true_x0_indices.min() < 0:
        print(f"!!! è­¦å‘Š: ç›®æ ‡ç´¢å¼•è¶Šç•Œï¼æœ€å¤§ç´¢å¼•: {true_x0_indices.max()}ï¼Œç±»åˆ«æ€»æ•°: {num_classes}")
    # --- è°ƒè¯•æ£€æŸ¥ç»“æŸ ---

    # 1. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤± (å¯¹åº”äº L_vlb çš„ä¸»è¦éƒ¨åˆ†å’Œ L_aux)
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_x0_indices, reduction='none')

    # [æ–°] å¢åŠ å¯¹æŸå¤±è¾“å‡ºæœ¬èº«çš„æ£€æŸ¥
    if torch.isnan(loss).any():
        print("!!! è­¦å‘Š: ç»è¿‡ cross_entropy è®¡ç®—åï¼ŒæŸå¤±å€¼ç«‹åˆ»å˜æˆäº† NaN !!!")

    
    # 2. æ ¹æ® D3PM æ··åˆæŸå¤± L_Î» çš„æ€æƒ³ï¼Œåº”ç”¨æƒé‡
    # L_Î» = L_vlb + Î» * [-log p(x0|xt)]
    # L_vlb çš„ KL é¡¹åœ¨ t>1 æ—¶æƒé‡ä¸º1ï¼Œåœ¨ t=1 æ—¶æƒé‡ä¸º1(é‡å»ºé¡¹)ã€‚
    # L_aux åœ¨æ‰€æœ‰ t ä¸Šæƒé‡éƒ½ä¸º Î»ã€‚
    # æ‰€ä»¥ï¼Œæ€»æƒé‡ä¸º 1 + Î»ï¼Œé™¤äº† t=0 çš„æƒ…å†µï¼ˆæˆ‘ä»¬ä¸å¤„ç†ï¼‰ã€‚
    
    # ä¸€ä¸ªéå¸¸å¸¸è§çš„ç®€åŒ–å®ç°æ˜¯ç›´æ¥åº”ç”¨æƒé‡
    # å¦ä¸€ç§æ¥è‡ªå…¶ä»–è®ºæ–‡çš„æ€è·¯æ˜¯ç»™ä½tçš„æŸå¤±æ›´é«˜çš„æƒé‡
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ L_Î» çš„ç²¾ç¥ï¼šä¸€ä¸ªåŸºç¡€æŸå¤± + ä¸€ä¸ªå°çš„è¾…åŠ©æŸå¤±
    
    # æƒé‡ä¸º 1(æ¥è‡ªL_vlb) + lambda_aux (æ¥è‡ªL_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # å¯¹äº t=1 çš„ç‰¹æ®Šæƒ…å†µï¼ŒVLBä¸­åªæœ‰é‡å»ºé¡¹ï¼Œå¯ä»¥è®¤ä¸ºæƒé‡ä¸åŒ
    # ä½†D3PMçš„æ··åˆæŸå¤±ç®€åŒ–äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¹Ÿé‡‡ç”¨ç®€åŒ–
    
    return final_loss.mean()

# 2.2 åŸå­åæ ‡æŸå¤±
# def calculate_coordinate_loss_wrapper(
#     predicted_r0: torch.Tensor,      # æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
#     true_noise: torch.Tensor,        # ç”¨äºç”Ÿæˆ r_t çš„çœŸå®é«˜æ–¯å™ªå£°, shape [M, 3]
#     r_t: torch.Tensor,               # è¾“å…¥åˆ°æ¨¡å‹çš„åŠ å™ªåæ ‡ (t>0), shape [M, 3]
#     t: torch.Tensor,                 # æ¯ä¸ªåæ ‡å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]
#     scheduler,                       # HierarchicalDiffusionScheduler å®ä¾‹
#     schedule_type: str               # ä½¿ç”¨çš„è°ƒåº¦ç±»å‹, 'alpha' æˆ– 'delta'
# ) -> torch.Tensor:
#     """
#     è®¡ç®—åŸå­åæ ‡çš„æŸå¤±ã€‚

#     è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œå®ƒæ¥æ”¶æ¨¡å‹é¢„æµ‹çš„ r0ï¼Œä½¿ç”¨è°ƒåº¦å™¨å°†å…¶è½¬æ¢ä¸º
#     é¢„æµ‹çš„å™ªå£° epsilonï¼Œç„¶åè®¡ç®—ä¸çœŸå®å™ªå£°çš„ L2 æŸå¤±ã€‚

#     Args:
#         predicted_r0: æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ã€‚
#         true_noise: çœŸå®çš„å™ªå£°ã€‚
#         r_t: åŠ å™ªåçš„åæ ‡ã€‚
#         t: æ—¶é—´æ­¥ã€‚
#         scheduler: å™ªå£°è°ƒåº¦å™¨å®ä¾‹ã€‚
#         schedule_type: ä½¿ç”¨çš„è°ƒåº¦ç±»å‹ ('alpha' æˆ– 'delta')ã€‚

#     Returns:
#         torch.Tensor: è®¡ç®—å‡ºçš„æ ‡é‡æŸå¤±å€¼ã€‚
#     """
#     # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚
#     # åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­è¢«æŸç§æ–¹å¼ç§»é™¤äº†ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œè¿™å¯ä»¥é˜²æ­¢å‡ºé”™ã€‚
#     if predicted_r0.shape[0] == 0:
#         return torch.tensor(0.0, device=predicted_r0.device)

#     # 2. è°ƒç”¨è°ƒåº¦å™¨çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä» predicted_r0 åæ¨å‡º predicted_noise
#     predicted_noise = scheduler.get_predicted_noise_from_r0(
#         r_t=r_t,
#         t=t,
#         predicted_r0=predicted_r0,
#         schedule_type=schedule_type
#     )

#     # 3. è®¡ç®—é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°ä¹‹é—´çš„ L2 æŸå¤± (å‡æ–¹è¯¯å·®, Mean Squared Error)
#     # F.mse_loss(A, B) ä¼šè®¡ç®— (A - B)^2 çš„æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ã€‚
#     loss = F.mse_loss(predicted_noise, true_noise)

def calculate_coordinate_loss_wrapper(
    predicted_x0: torch.Tensor,      # æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
    true_x0: torch.Tensor,           # çœŸå®çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
    r_t: torch.Tensor,               # è¾“å…¥åˆ°æ¨¡å‹çš„åŠ å™ªåæ ‡ (t>0), shape [M, 3]
    t: torch.Tensor,                 # æ¯ä¸ªåæ ‡å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]
    scheduler,                       # HierarchicalDiffusionScheduler å®ä¾‹
    schedule_type: str               # ä½¿ç”¨çš„è°ƒåº¦ç±»å‹, 'alpha' æˆ– 'delta'
) -> torch.Tensor:
    """
    è®¡ç®—åŸå­åæ ‡çš„æŸå¤±ã€‚

    è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œå®ƒæ¥æ”¶æ¨¡å‹é¢„æµ‹çš„å™ªå£°ï¼Œç„¶åè®¡ç®—ä¸çœŸå®å™ªå£°çš„ L2 æŸå¤±ã€‚

    Args:
        predicted_x0: æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ x0ã€‚
        true_x0: çœŸå®çš„å¹²å‡€åæ ‡ x0ã€‚

    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æ ‡é‡æŸå¤±å€¼ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚
    if predicted_x0.shape[0] == 0:
        return torch.tensor(0.0, device=predicted_x0.device)

    # 2. è®¡ç®—é¢„æµ‹çš„ x0 å’ŒçœŸå®çš„ x0 ä¹‹é—´çš„ L2 æŸå¤± (å‡æ–¹è¯¯å·®, Mean Squared Error)
    loss = F.mse_loss(predicted_x0, true_x0)

    return loss

# 2.3 è¾¹ç±»å‹æŸå¤±
def calculate_bond_type_loss(
    pred_logits: torch.Tensor,      # æ¨¡å‹å¯¹å¹²å‡€è¾¹ç±»å‹çš„é¢„æµ‹ logits, shape [M_edges, C_bonds]
    true_b0_indices: torch.Tensor, # çœŸå®çš„å¹²å‡€è¾¹ç±»å‹ç´¢å¼•, shape [M_edges]
    lambda_aux: float = 0.001,     # è¾…åŠ©æŸå¤±çš„æƒé‡
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆè¾¹ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ï¼Œä¸åŸå­ç±»å‹æŸå¤±çš„é€»è¾‘å®Œå…¨ç›¸åŒã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_b0_indices: çœŸå®çš„è¾¹ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªè¾¹å¯¹åº”çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ (å½“å‰æœªä½¿ç”¨ï¼Œä¸ºæœªæ¥æ‰©å±•ä¿ç•™)ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚å¦‚æœä¸€ä¸ªæ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦é¢„æµ‹çš„è¾¹ï¼Œåˆ™æŸå¤±ä¸º0ã€‚
    # è¿™åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­æ˜¯å­¤ç«‹ç‚¹æ—¶å¯èƒ½å‘ç”Ÿã€‚
    if pred_logits.shape[0] == 0:
        return torch.tensor(0.0, device=pred_logits.device)
        
    # 2. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯æ¡è¾¹è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_b0_indices, reduction='none')
    
    # 3. åº”ç”¨ D3PM æ··åˆæŸå¤±çš„ç®€åŒ–æƒé‡
    # æ€»æƒé‡ = 1 (æ¥è‡ª L_vlb) + lambda_aux (æ¥è‡ª L_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # 4. å¯¹æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è¾¹çš„æŸå¤±æ±‚å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ ‡é‡æŸå¤±
    return final_loss.mean()


# ==============================================================================
# 3. éªŒè¯å‡½æ•° (Validation Function) - [ä¿®æ”¹åç‰ˆæœ¬]
# ==============================================================================
@torch.no_grad() # è£…é¥°å™¨ï¼Œè¡¨ç¤ºè¯¥å‡½æ•°å†…æ‰€æœ‰ torch è®¡ç®—éƒ½ä¸éœ€è¦è®°å½•æ¢¯åº¦
def validate(val_loader, model, scheduler, args, amp_autocast):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æŸå¤±ã€‚
    æ­¤å‡½æ•°çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—é€»è¾‘ä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€è‡´ã€‚
    """
    device = args.device
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_val_loss = 0.0
    pbar_val = tqdm(val_loader, desc=f"Validating", leave=False)

    for clean_batch in pbar_val:
        clean_batch = clean_batch.to(device)

        # with amp_autocast():
        # --- [é€»è¾‘ä¸è®­ç»ƒå¾ªç¯å®Œå…¨ç›¸åŒ] ---

        # --- 0. å‡†å¤‡å·¥ä½œ ---
        num_graphs, num_nodes, num_edges = clean_batch.num_graphs, clean_batch.num_nodes, clean_batch.num_edges
        # scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
        scaled_pos = clean_batch.pos # ä¸è¿›è¡Œåæ ‡ç¼©æ”¾
        t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device)
        t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
        noise1, noise2 = torch.randn_like(scaled_pos), torch.randn_like(scaled_pos)
        t1_per_node, t1_per_edge = t1[clean_batch.batch], t1[clean_batch.batch[clean_batch.edge_index[0]]]

        # --- ç­–ç•¥ I: å…¨å±€å»å™ª ---
        noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, 'alpha')
        noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
        noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
        noised_data_I = clean_batch.clone(); noised_data_I.pos, noised_data_I.x, noised_data_I.edge_attr = noised_pos_I, noised_x_I, noised_edge_attr_I
        
        target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
        target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)
        
        predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)
        
        lossI_a = calculate_atom_type_loss(predictions_I['atom_type_logits'], clean_batch.x.argmax(dim=-1), args.lambda_aux)
        lossI_r = calculate_coordinate_loss_wrapper(predictions_I['predicted_r0'], scaled_pos, noised_pos_I, t1_per_node, scheduler, 'alpha')
        lossI_b = calculate_bond_type_loss(predictions_I['bond_logits'], clean_batch.edge_attr.argmax(dim=-1), args.lambda_aux)
        loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b

        # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ ---
        target_node_mask_II = clean_batch.is_new_node.squeeze().bool()
        context_node_mask_II = ~target_node_mask_II
        target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
        context_edge_mask = ~target_edge_mask
        
        t_T1_per_node, t_T1_per_edge = torch.full_like(t1_per_node, scheduler.T1), torch.full_like(t1_per_edge, scheduler.T1)
        t2_per_node, t2_per_edge = t2[clean_batch.batch], t2[clean_batch.batch[clean_batch.edge_index[0]]]

        noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
        noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
        noised_pos_II = torch.zeros_like(scaled_pos); noised_pos_II[context_node_mask_II], noised_pos_II[target_node_mask_II] = noised_pos_context, noised_pos_target
        
        noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
        noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
        noised_x_II = torch.zeros_like(clean_batch.x); noised_x_II[context_node_mask_II], noised_x_II[target_node_mask_II] = noised_x_context, noised_x_target
    
        noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
        noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
        noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr); noised_edge_attr_II[context_edge_mask], noised_edge_attr_II[target_edge_mask] = noised_edge_attr_context, noised_edge_attr_target
    
        noised_data_II = clean_batch.clone(); noised_data_II.pos, noised_data_II.x, noised_data_II.edge_attr = noised_pos_II, noised_x_II, noised_edge_attr_II
    
        predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

        lossII_a = calculate_atom_type_loss(predictions_II['atom_type_logits'], clean_batch.x[target_node_mask_II].argmax(dim=-1), args.lambda_aux)
        lossII_r = calculate_coordinate_loss_wrapper(predictions_II['predicted_r0'], scaled_pos[target_node_mask_II], noised_pos_target, t2_per_node[target_node_mask_II], scheduler, 'delta')
        lossII_b = calculate_bond_type_loss(predictions_II['bond_logits'], clean_batch.edge_attr[target_edge_mask].argmax(dim=-1), args.lambda_aux)
        loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b

        # --- æ€»éªŒè¯æŸå¤± ---
        # total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II 
        total_loss = loss_I + loss_II
        
        total_val_loss += total_loss.item()
        pbar_val.set_postfix({
            'val_loss': total_loss.item(),
            'val_loss_I': loss_I.item(),
            'val_loss_II': loss_II.item()
        })
    
    avg_val_loss = total_val_loss / len(val_loader)
    return avg_val_loss

# ==============================================================================
# 4. ä¸»è®­ç»ƒå‡½æ•° (Main Training Function)
# ==============================================================================

def train(
    args, # åŒ…å«è¶…å‚æ•°çš„å¯¹è±¡ï¼Œå¦‚å­¦ä¹ ç‡ã€epochæ•°ç­‰
    logger,
    train_loader, # è®­ç»ƒæ•°æ®
    val_loader, # æµ‹è¯•æ•°æ®
    model: nn.Module, # E_DiT_Network å®ä¾‹
    scheduler, # HierarchicalDiffusionScheduler å®ä¾‹
    amp_autocast,
    loss_scaler,
    train_sampler
):
    """
    ä¸»è®­ç»ƒå‡½æ•°ã€‚
    """
    device = args.device
    model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    T_max = args.epochs  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸ºæ€» epoch æ•°
    lr_min_factor = args.lr_min_factor
    scheduler_model = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=lr_min_factor * args.learning_rate)

    best_val_loss = float('inf')
    best_epoch = 0
    
    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨: {args.checkpoints_dir}")
    logger.info("å¼€å§‹è®­ç»ƒ...")

    accumulation_steps = args.accumulation_steps

    for epoch in range(1, args.epochs + 1):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºâ€œè®­ç»ƒæ¨¡å¼â€
        # å®ƒä¼šé€šçŸ¥æ¨¡å‹ä¸­æ‰€æœ‰å…·æœ‰ä¸åŒè®­ç»ƒ/è¯„ä¼°è¡Œä¸ºçš„å±‚ï¼ˆä¸»è¦æ˜¯ Dropout å±‚å’Œ BatchNorm å±‚ï¼‰åˆ‡æ¢åˆ°å®ƒä»¬çš„è®­ç»ƒçŠ¶æ€ã€‚
        # Dropout å±‚åœ¨è®­ç»ƒæ—¶ä¼šéšæœºâ€œä¸¢å¼ƒâ€ä¸€äº›ç¥ç»å…ƒï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¸ä¼šä¸¢å¼ƒï¼Œä¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒã€‚
        # BatchNorm å±‚åœ¨è®­ç»ƒæ—¶ä¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ›´æ–°å…¶å†…éƒ¨çš„å…¨å±€ç»Ÿè®¡é‡ï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¼šä½¿ç”¨å·²å­¦ä¹ åˆ°çš„å…¨å±€ç»Ÿè®¡é‡ã€‚
        model.train()  

        if args.distributed:
            train_sampler.set_epoch(epoch)

        # åˆå§‹åŒ–ä¸€ä¸ªå˜é‡ï¼Œç”¨äºç´¯åŠ å½“å‰è¿™ä¸ª epoch å†…æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±å€¼
        # åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ total_loss_epoch é™¤ä»¥æ‰¹æ¬¡çš„æ€»æ•°ï¼Œæ¥è®¡ç®—å¹¶æ‰“å°å‡ºè¿™ä¸ª epoch çš„å¹³å‡æŸå¤±ï¼Œä»¥æ­¤æ¥ç›‘æ§è®­ç»ƒçš„è¿›å±•ã€‚
        total_loss_epoch = 0.0
        total_loss_I_epoch = 0.0
        total_loss_II_epoch = 0.0
        total_lossI_a_epoch = 0.0
        total_lossI_r_epoch = 0.0
        total_lossI_b_epoch = 0.0
        total_lossII_a_epoch = 0.0
        total_lossII_r_epoch = 0.0
        total_lossII_b_epoch = 0.0

        optimizer.zero_grad()

        # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{args.epochs}")  # desc=...: è®¾ç½®è¿›åº¦æ¡å·¦ä¾§çš„æè¿°æ€§æ–‡å­—ï¼Œä¾‹å¦‚ Epoch 1/100
       
        optimizer.zero_grad()
        for i, clean_batch in enumerate(pbar):

            clean_batch = clean_batch.to(device)

            is_sync_step = ((i + 1) % accumulation_steps == 0) or (i + 1 == len(train_loader))

            from contextlib import suppress
            context = model.no_sync() if (args.distributed and not is_sync_step) else suppress()
            with context:

                # with amp_autocast():
                # --- 0. å‡†å¤‡å·¥ä½œ ---
                num_graphs = clean_batch.num_graphs # æ‰¹æ¬¡ä¸­åŒ…å«çš„ç‹¬ç«‹å›¾çš„æ•°é‡ï¼ˆç­‰äº batch_sizeï¼‰ã€‚ç”¨äºé‡‡æ ·å›¾çº§åˆ«çš„å˜é‡ï¼Œå¦‚æ—¶é—´æ­¥ t
                num_nodes = clean_batch.num_nodes #  æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„èŠ‚ç‚¹æ€»æ•°
                num_edges = clean_batch.num_edges # æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„è¾¹æ€»æ•°

                # a. åæ ‡ç¼©æ”¾
                # scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
                scaled_pos = clean_batch.pos # ä¸è¿›è¡Œåæ ‡ç¼©æ”¾
            
                # b. é‡‡æ ·æ—¶é—´æ­¥å’Œé«˜æ–¯å™ªå£°
                # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªå›¾éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ t1
                # t1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [batch_size] çš„å¼ é‡ï¼Œä¾‹å¦‚ tensor([18, 98, 21, ...])
                t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device) 
                t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
                # noise1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, 3] çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ã€‚noise1[i] å°±æ˜¯è¦åŠ åˆ°ç¬¬ i ä¸ªåŸå­åæ ‡ä¸Šçš„å™ªå£°å‘é‡ã€‚
                noise1 = torch.randn_like(scaled_pos)
                noise2 = torch.randn_like(scaled_pos)
            
                # c. å°† per-graph çš„æ—¶é—´æ­¥æ‰©å±•åˆ° per-node å’Œ per-edge
                # t1: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_graphs] çš„å¼ é‡ã€‚å‡è®¾ batch_size=4ï¼Œt1å¯èƒ½é•¿è¿™æ ·ï¼štensor([18, 98, 21, 76])
                # clean_batch.batch: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_nodes] çš„å¼ é‡ï¼Œè®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªä¸ªå›¾ã€‚
                # å®ƒå¯èƒ½é•¿è¿™æ ·ï¼ˆå‡è®¾4ä¸ªå›¾åˆ†åˆ«æœ‰3, 2, 4, 3ä¸ªèŠ‚ç‚¹ï¼‰ï¼štensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3])ã€‚
                t1_per_node = t1[clean_batch.batch] # å½¢çŠ¶ä¸º[num_nodes]
                t1_per_edge = t1[clean_batch.batch[clean_batch.edge_index[0]]] # å½¢çŠ¶ä¸º[num_edges]


                # --- ç­–ç•¥ I: å…¨å±€å»å™ª (ç”Ÿæˆå™ªå£°å›¾ â… ) ---
            
                # a. åŠ å™ªåæ ‡
                noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, schedule_type='alpha')  # é˜…è¯»æ ‡è®°
            
                # b. åŠ å™ªåŸå­ç±»å‹
                noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            
                # c. åŠ å™ªè¾¹å±æ€§
                noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â… 
                # å¤åˆ¶å¹²å‡€çš„æ•°æ®ï¼Œæ›´æ”¹åŠ å™ªçš„éƒ¨åˆ†
                noised_data_I = clean_batch.clone()
                noised_data_I.pos = noised_pos_I
                noised_data_I.x = noised_x_I
                noised_data_I.edge_attr = noised_edge_attr_I
            
                # e. å‡†å¤‡æ¨¡å‹è¾“å…¥
                # åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºå½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰åŸå­çš„æ€»æ•°ï¼Œå†…å®¹å…¨ä¸º True çš„å‘é‡ã€‚
                target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
                # å¤„ç†å¹¶è¾“å‡ºæ‰€æœ‰è¾¹çš„é¢„æµ‹ç»“æœ
                target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)

                # f. æ¨¡å‹å‰å‘ä¼ æ’­
                predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)
            
                # g. è®¡ç®—æŸå¤± â… 
                lossI_a = calculate_atom_type_loss(
                    predictions_I['atom_type_logits'],
                    clean_batch.x.argmax(dim=-1),  # ä» One-Hot ç¼–ç çš„ç‰¹å¾å¼ é‡ä¸­ï¼Œæå–å‡ºæ¯ä¸ªé¡¹ç›®å¯¹åº”çš„ç±»åˆ«ç´¢å¼• (class index)
                    args.lambda_aux
                )
                lossI_r = calculate_coordinate_loss_wrapper(
                    predicted_x0=predictions_I['predicted_r0'], 
                    true_x0=scaled_pos, 
                    r_t=noised_pos_I, 
                    t=t1_per_node, 
                    scheduler=scheduler, 
                    schedule_type='alpha'
                )
                lossI_b = calculate_bond_type_loss(
                    pred_logits=predictions_I['bond_logits'], 
                    true_b0_indices=clean_batch.edge_attr.argmax(dim=-1),
                    lambda_aux=args.lambda_aux
                )
                loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b


                # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ (ç”Ÿæˆå™ªå£°å›¾ â…¡) ---

                # a. è¯†åˆ«ä¸Šä¸‹æ–‡å’Œç›®æ ‡
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç›®æ ‡
                target_node_mask_II = clean_batch.is_new_node.squeeze() # is_new_node å°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡maskï¼Œç»´åº¦å‹ç¼©ä¸º[num_nodes]
                target_node_mask_II = target_node_mask_II.to(torch.bool)
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯ä¸Šä¸‹æ–‡èŠ‚ç‚¹ï¼Œç”¨äºå¯¹ä¸Šä¸‹æ–‡èŠ‚ç‚¹åŠ å™ª
                context_node_mask_II = ~target_node_mask_II
                # æ ‡è¯†å“ªäº›è¾¹æ˜¯ä¸é¢„æµ‹ç›®æ ‡èŠ‚ç‚¹ç›¸å…³çš„è¾¹
                # å¯¹äºç¬¬ i æ¡è¾¹ï¼Œå¦‚æœå®ƒçš„èµ·ç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹æˆ–è€…å®ƒçš„ç»ˆç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯éœ€è¦è¢«é¢„æµ‹çš„è¾¹
                target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
                # ç”¨äºå¯¹ä¸Šä¸‹æ–‡è¾¹åŠ å™ª
                context_edge_mask = ~target_edge_mask

                # b. å‡†å¤‡æ—¶é—´æ­¥ (T1 å’Œ t2)
                # åˆ›å»ºä¸€ä¸ªä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒã€ç±»å‹ç›¸åŒã€è®¾å¤‡ç›¸åŒçš„æ–°å¼ é‡ï¼Œå¹¶å°†æ‰€æœ‰å…ƒç´ å¡«å……ä¸ºT1
                t_T1_per_node = torch.full_like(t1_per_node, fill_value=scheduler.T1)
                t_T1_per_edge = torch.full_like(t1_per_edge, fill_value=scheduler.T1)
                t2_per_node = t2[clean_batch.batch]
                t2_per_edge = t2[clean_batch.batch[clean_batch.edge_index[0]]]

                # c. å¯¹ä¸Šä¸‹æ–‡å’Œç›®æ ‡åˆ†åˆ«åŠ å™ª
                # åæ ‡
                # è®¡ç®—å‡ºæ‰€æœ‰ä¸Šä¸‹æ–‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
                # è®¡ç®—å‡ºæ‰€æœ‰ç›®æ ‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
                # åˆ›å»ºä¸€ä¸ªç©ºçš„â€œç”»å¸ƒâ€
                noised_pos_II = torch.zeros_like(scaled_pos)
                # å°†è®¡ç®—å¥½çš„ä¸Šä¸‹æ–‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[context_node_mask_II] = noised_pos_context
                # å°†è®¡ç®—å¥½çš„ç›®æ ‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[target_node_mask_II] = noised_pos_target

                # åŸå­ç±»å‹
                noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
                noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
                noised_x_II = torch.zeros_like(clean_batch.x)
                noised_x_II[context_node_mask_II] = noised_x_context
                noised_x_II[target_node_mask_II] = noised_x_target
            
                # è¾¹å±æ€§
                noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
                noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
                noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr)
                noised_edge_attr_II[context_edge_mask] = noised_edge_attr_context
                noised_edge_attr_II[target_edge_mask] = noised_edge_attr_target
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â…¡
                noised_data_II = clean_batch.clone()
                noised_data_II.pos = noised_pos_II
                noised_data_II.x = noised_x_II
                noised_data_II.edge_attr = noised_edge_attr_II

            
                # f. æ¨¡å‹å‰å‘ä¼ æ’­ (æ³¨æ„æ—¶é—´æ­¥ä¼ å…¥çš„æ˜¯ t2)
                predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

                # with torch.no_grad(): # ç¡®ä¿è¿™éƒ¨åˆ†ä¸è®¡ç®—æ¢¯åº¦
                #     # --- ç›‘æ§ç­–ç•¥ I (å…¨å±€å¾®è°ƒ) ---
                #     # 1. è·å–æ¨¡å‹é¢„æµ‹çš„å™ªå£°
                #     predicted_noise_I = predictions_I['predicted_r0']
                    
                #     # 2. æ ¹æ®é¢„æµ‹çš„å™ªå£°ï¼Œåæ¨å‡ºæ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ x_hat_0
                #     alpha_bar_t_I = scheduler.alpha_bars[t1_per_node]
                #     sqrt_alpha_bar_t_I = torch.sqrt(alpha_bar_t_I).unsqueeze(1)
                #     sqrt_one_minus_alpha_bar_t_I = torch.sqrt(1.0 - alpha_bar_t_I).unsqueeze(1)
                #     predicted_x0_I = (noised_pos_I - sqrt_one_minus_alpha_bar_t_I * predicted_noise_I) / sqrt_alpha_bar_t_I
                    
                #     # 3. è®¡ç®—å¹¶è®°å½•å…³é”®æŒ‡æ ‡çš„æ¨¡é•¿ (Norm)
                #     # æˆ‘ä»¬å…³å¿ƒçš„æ˜¯å¹³å‡èŒƒæ•°ï¼Œè€Œä¸æ˜¯æ€»èŒƒæ•°
                #     norm_true_noise_I = torch.linalg.norm(noise1, dim=-1).mean()
                #     norm_predicted_noise_I = torch.linalg.norm(predicted_noise_I, dim=-1).mean()
                #     norm_predicted_x0_I = torch.linalg.norm(predicted_x0_I, dim=-1).mean()
                #     norm_true_x0_I = torch.linalg.norm(scaled_pos, dim=-1).mean() # scaled_pos æ˜¯çœŸå®çš„å¹²å‡€åæ ‡

                #     # --- ç›‘æ§ç­–ç•¥ II (å±€éƒ¨ç”Ÿæˆ) ---
                #     predicted_noise_II = predictions_II['predicted_r0']
                    
                #     alpha_bar_t_II = scheduler.delta_bars[t2_per_node[target_node_mask_II]]
                #     sqrt_alpha_bar_t_II = torch.sqrt(alpha_bar_t_II).unsqueeze(1)
                #     sqrt_one_minus_alpha_bar_t_II = torch.sqrt(1.0 - alpha_bar_t_II).unsqueeze(1)
                #     predicted_x0_II = (noised_pos_target - sqrt_one_minus_alpha_bar_t_II * predicted_noise_II) / sqrt_alpha_bar_t_II

                #     norm_true_noise_II = torch.linalg.norm(noise2[target_node_mask_II], dim=-1).mean()
                #     norm_predicted_noise_II = torch.linalg.norm(predicted_noise_II, dim=-1).mean()
                #     norm_predicted_x0_II = torch.linalg.norm(predicted_x0_II, dim=-1).mean()
                #     norm_true_x0_II = torch.linalg.norm(scaled_pos[target_node_mask_II], dim=-1).mean()

                # g. è®¡ç®—æŸå¤± â…¡
                # æ³¨æ„ï¼šè¿™é‡Œçš„çœŸå®æ ‡ç­¾å’Œå™ªå£°éƒ½éœ€è¦æ ¹æ® mask è¿›è¡Œç­›é€‰
                lossII_a = calculate_atom_type_loss(
                    predictions_II['atom_type_logits'],
                    clean_batch.x[target_node_mask_II].argmax(dim=-1),
                    args.lambda_aux
                )
                lossII_r = calculate_coordinate_loss_wrapper(
                    predicted_x0=predictions_II['predicted_r0'], 
                    true_x0=scaled_pos[target_node_mask_II], 
                    r_t=noised_pos_target, 
                    t=t2_per_node[target_node_mask_II], 
                    scheduler=scheduler, 
                    schedule_type='delta'
                )
                lossII_b = calculate_bond_type_loss(
                    pred_logits=predictions_II['bond_logits'],
                    true_b0_indices=clean_batch.edge_attr[target_edge_mask].argmax(dim=-1),
                    lambda_aux=args.lambda_aux
                )
                loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b


                # --- æ€»æŸå¤±ä¸åå‘ä¼ æ’­ ---
                # total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II
                total_loss = loss_I + loss_II

                total_loss = total_loss / accumulation_steps
                total_loss.backward()

        

            if is_sync_step:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                # æ›´æ–°å®Œæˆåï¼Œæ¸…ç©ºæ¢¯åº¦ï¼Œä¸ºä¸‹ä¸€ä¸ªç´¯ç§¯å‘¨æœŸåšå‡†å¤‡
                optimizer.zero_grad()
                
            total_loss_epoch += total_loss.item()
            total_loss_I_epoch += loss_I.item()
            total_loss_II_epoch += loss_II.item()
            total_lossI_a_epoch += lossI_a.item()
            total_lossI_r_epoch += lossI_r.item()
            total_lossI_b_epoch += lossI_b.item()
            total_lossII_a_epoch += lossII_a.item()
            total_lossII_r_epoch += lossII_r.item()
            total_lossII_b_epoch += lossII_b.item()
            pbar.set_postfix({
                'loss': total_loss.item() * accumulation_steps, # å°†å½“å‰æ‰¹æ¬¡çš„æŸå¤±ä¹˜ä»¥ accumulation_stepsï¼Œå¾—åˆ°å¯æ¯”è¾ƒçš„çœŸå®æŸå¤±
                'loss_I': loss_I.item(), # å°† loss_I çš„æ•°å€¼æ·»åŠ åˆ°è¿›åº¦æ¡
                'loss_II': loss_II.item(), # å°† loss_II çš„æ•°å€¼æ·»åŠ åˆ°è¿›åº¦æ¡
                'lossI_a': lossI_a.item(), # åŸå­ç±»å‹æŸå¤± â… 
                'lossI_r': lossI_r.item(), # åæ ‡æŸå¤± â… 
                'lossI_b': lossI_b.item(), # è¾¹ç±»å‹æŸå¤± â… 
                'lossII_a': lossII_a.item(), # åŸå­ç±»å‹æŸå¤± â…¡
                'lossII_r': lossII_r.item(), # åæ ‡æŸå¤± â…¡
                'lossII_b': lossII_b.item()#ï¼Œ  # è¾¹ç±»å‹æŸå¤± â…¡
                # 'p_noise_I_norm': norm_predicted_noise_I.item(),
                # 'p_x0_I_norm': norm_predicted_x0_I.item(),
                # 't_x0_I_norm': norm_true_x0_I.item(), # çœŸå® x0 çš„ Normï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªå°å¸¸æ•°
            })
            
        avg_scaled_train_loss = total_loss_epoch / len(train_loader)
        # å†ä¹˜ä»¥ accumulation_stepsï¼Œå¾—åˆ°å¯æ¯”è¾ƒçš„çœŸå®å¹³å‡æŸå¤±
        avg_real_train_loss = avg_scaled_train_loss * accumulation_steps
        logger.info(f"Epoch {epoch} [Train] å®Œæˆ, å¹³å‡æŸå¤±: {avg_real_train_loss:.4f}")
        num_batches = len(train_loader)
        avg_loss_I = total_loss_I_epoch / num_batches
        avg_loss_II = total_loss_II_epoch / num_batches
        avg_lossI_a = total_lossI_a_epoch / num_batches
        avg_lossI_r = total_lossI_r_epoch / num_batches
        avg_lossI_b = total_lossI_b_epoch / num_batches
        avg_lossII_a = total_lossII_a_epoch / num_batches
        avg_lossII_r = total_lossII_r_epoch / num_batches
        avg_lossII_b = total_lossII_b_epoch / num_batches

        # æ„å»ºæ—¥å¿—å­—ç¬¦ä¸²
        log_str = (
            f"  -> Loss Details: loss={avg_real_train_loss:.2e}, " # ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•
            f"loss_I={avg_loss_I:.2f}, loss_II={avg_loss_II:.2f}, "
            f"lossI_a={avg_lossI_a:.2f}, lossI_r={avg_lossI_r:.2f}, lossI_b={avg_lossI_b:.2f}, "
            f"lossII_a={avg_lossII_a:.2f}, lossII_r={avg_lossII_r:.2f}, lossII_b={avg_lossII_b:.2f}"
        )
        logger.info(log_str)


        # --- éªŒè¯é˜¶æ®µ ---
        if epoch >= args.val_thre and (epoch % args.val_log_freq == 0):
            avg_val_loss = validate(val_loader, model, scheduler, args, amp_autocast)
            logger.info(f"Epoch {epoch} [Validation] å®Œæˆ, å¹³å‡æŸå¤±: {avg_val_loss:.4f}")

            # ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹ 
            logger.info(f"åœ¨ Epoch {epoch} ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹åŠå…¶éªŒè¯æŸå¤±...")
            if args.distributed:
                model_state_to_save = model.module.state_dict()
            else:
                model_state_to_save = model.state_dict()

            checkpoint_state = {
                'epoch': epoch,
                'model_state_dict': model_state_to_save,
                'optimizer_model_state_dict': optimizer.state_dict(),
                'scheduler_model_state_dict': scheduler_model.state_dict(),
                'validation_loss': avg_val_loss, # <-- æ˜ç¡®ä¿å­˜å½“å‰ epoch çš„éªŒè¯æŸå¤±
                'args': args
            }
            if loss_scaler is not None:
                checkpoint_state['loss_scaler_state_dict'] = loss_scaler.state_dict()
            # ä½¿ç”¨åŒ…å« epoch ç¼–å·çš„å”¯ä¸€æ–‡ä»¶å
            checkpoint_path = os.path.join(args.checkpoints_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint_state, checkpoint_path)

            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_epoch = epoch
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜æœ€ä½³æ¨¡å‹...")
                
                if args.distributed:
                    model_state_to_save = model.module.state_dict()
                else:
                    model_state_to_save = model.state_dict()

                # ä¸ºæœ€ä½³æ¨¡å‹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ä¿å­˜çŠ¶æ€
                best_model_state = {
                    'epoch': epoch,
                    'model_state_dict': model_state_to_save,
                    'best_val_loss': best_val_loss,
                    'args': args
                }
                
                best_model_path = os.path.join(args.checkpoints_dir, 'best_model.pth')
                torch.save(best_model_state, best_model_path)
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_model.step()
            
    logger.info("è®­ç»ƒå®Œæˆã€‚")
    logger.info(f"æœ€ç»ˆï¼Œæœ€ä½³æ¨¡å‹å‘ç°åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º: {best_val_loss:.4f}")