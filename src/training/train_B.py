import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR  # å¯¼å…¥ CosineAnnealingLR
import os



# ==============================================================================
# 1. è¾…åŠ©å‡½æ•° (Helper Functions)
# ==============================================================================

def scale_to_unit_sphere(pos: torch.Tensor, batch_map: torch.Tensor) -> torch.Tensor:
    """
    å°†æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾çš„åæ ‡ç‹¬ç«‹åœ°ç¼©æ”¾åˆ°å•ä½çƒå†…ã€‚
    
    Args:
        pos (torch.Tensor): æ‰¹æ¬¡ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åæ ‡å¼ é‡, shape [N, 3]ã€‚
        batch_map (torch.Tensor): å°†æ¯ä¸ªèŠ‚ç‚¹æ˜ å°„åˆ°å…¶æ‰€å±å›¾çš„å‘é‡, shape [N]ã€‚
    
    Returns:
        torch.Tensor: ç¼©æ”¾åçš„åæ ‡ã€‚
    """
    # å‡è®¾æˆ‘ä»¬çš„è¾“å…¥ pos æ˜¯ä¸€ä¸ª [N, 3] çš„å¼ é‡ï¼Œä»£è¡¨æ‰¹æ¬¡ä¸­æ‰€æœ‰ N ä¸ªåŸå­çš„åæ ‡ï¼›
    # batch_map æ˜¯ä¸€ä¸ª [N] çš„å¼ é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªåŸå­å±äºå“ªä¸ªåˆ†å­å›¾ï¼ˆ0, 0, 0, 1, 1, ...ï¼‰ã€‚
    # PyG çš„ scatter å‡½æ•°å¯ä»¥é«˜æ•ˆåœ°æŒ‰ç»„æ±‚å’Œ/æ±‚å‡å€¼
    from torch_geometric.utils import scatter
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åˆ°å…¶è´¨å¿ƒçš„è·ç¦»
    # è¾“å…¥ pos çš„ç»´åº¦æ˜¯ [N, 3]
    # e.g. å¯¹äºç¬¬ 0 è¡Œ [x_0, y_0, z_0]ï¼Œå®ƒè®¡ç®— sqrt(x_0Â² + y_0Â² + z_0Â²)
    # torch.linalg.norm æ²¿ç€ dim=1 è¿›è¡Œæ“ä½œï¼Œè¿™ä¸ªç»´åº¦åœ¨è®¡ç®—åä¼šæ¶ˆå¤±
    # è¾“å‡º distances çš„ç»´åº¦æ˜¯ [N]
    distances = torch.linalg.norm(pos, dim=1)
    
    # æŒ‰å›¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå›¾ä¸­çš„æœ€å¤§è·ç¦»
    max_distances = scatter(distances, batch_map, dim=0, reduce='max') # shape: [num_graphs]
    
    # è®¡ç®—æ¯ä¸ªå›¾çš„ç¼©æ”¾å› å­ï¼ŒåŠ ä¸Šä¸€ä¸ªå°çš„ epsilon é˜²æ­¢é™¤ä»¥é›¶
    scale_factors = max_distances[batch_map].unsqueeze(1) + 1e-8

    
    # ç¼©æ”¾åæ ‡
    return pos / scale_factors


def noise_discrete_features(
    features_0: torch.Tensor,
    Q_bar: torch.Tensor,
    t_per_item: torch.Tensor
) -> torch.Tensor:
    """
    å¯¹ one-hot ç¼–ç çš„ç¦»æ•£ç‰¹å¾ï¼ˆå¦‚åŸå­ç±»å‹ã€è¾¹ç±»å‹ï¼‰è¿›è¡ŒåŠ å™ªã€‚

    Args:
        features_0 (torch.Tensor): å¹²å‡€çš„ one-hot ç‰¹å¾, shape [M, K] (Mä¸ªé¡¹ç›®, Kä¸ªç±»åˆ«)ã€‚
        Q_bar (torch.Tensor):      è½¬ç§»çŸ©é˜µé›†åˆ, shape [T, K, K] (Tä¸ªæ—¶é—´æ­¥)ã€‚
        t_per_item (torch.Tensor): æ¯ä¸ªé¡¹ç›®å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]ã€‚

    Returns:
        torch.Tensor: åŠ å™ªåçš„ one-hot ç‰¹å¾ã€‚
    """
    # 1. æ ¹æ®æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ tï¼Œä» Q_bar ä¸­é€‰å‡ºå¯¹åº”çš„è½¬ç§»çŸ©é˜µ
    # Q_bar_t çš„ shape ä¸º [M, K, K]
    Q_bar_t = Q_bar[t_per_item] 
    
    # 2. è®¡ç®—åŠ å™ªåçš„æ¦‚ç‡åˆ†å¸ƒ
    # features_0.unsqueeze(1) -> [M, 1, K]   åœ¨ features_0 çš„ç¬¬1ä¸ªç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
    # Q_bar_t                 -> [M, K, K]
    # prob_t                  -> [M, 1, K]
    # æ‰§è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
    prob_t = torch.bmm(features_0.unsqueeze(1), Q_bar_t).squeeze(1) # shape: [M, K]
    
    # 3. æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°æ–°çš„ç±»åˆ«ç´¢å¼•
    # torch.multinomial è¦æ±‚è¾“å…¥æ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬è¿™é‡Œå·²ç»æ˜¯æ¦‚ç‡äº†
    # multinomial ä¼šæŠŠæ¯ä¸€è¡Œéƒ½çœ‹ä½œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„â€œéª°å­â€çš„æ¦‚ç‡è®¾ç½®
    # num_samples=1 æŒ‡æ¯è¡Œé‡‡æ ·ä¸€æ¬¡ï¼Œè¿™æ—¶è¾“å‡ºç»´åº¦ä¸º[M, 1]
    # .squeeze(-1) ä½œç”¨ä¸ºç§»é™¤æœ€åä¸€ä¸ªç»´åº¦ï¼ˆdim=-1ï¼‰ä¸Šå¤§å°ä¸º1çš„ç»´åº¦
    sampled_indices = torch.multinomial(prob_t, num_samples=1).squeeze(-1) # shape: [M]
    
    # 4. å°†é‡‡æ ·å‡ºçš„ç´¢å¼•è½¬æ¢å› one-hot ç¼–ç 
    num_classes = features_0.shape[1]  # è·å–ç±»åˆ«çš„æ€»æ•°
    # å°†æ•´æ•°ç±»åˆ«ç´¢å¼•è½¬æ¢æˆ One-Hot ç¼–ç å‘é‡
    features_t = torch.nn.functional.one_hot(sampled_indices, num_classes=num_classes).float()
    
    return features_t

# ==============================================================================
# 2. æŸå¤±å‡½æ•°æ¡†æ¶ (Loss Function Skeletons)
# ==============================================================================

# 2.1 åŸå­ç±»å‹æŸå¤±
# æ¨èä½¿ç”¨çš„ã€æ›´ç®€æ´çš„æŸå¤±å‡½æ•°
def calculate_atom_type_loss(
    pred_logits: torch.Tensor,   # æ¨¡å‹å¯¹ x0 çš„é¢„æµ‹ logits, shape [M, C]
    true_x0_indices: torch.Tensor, # çœŸå®çš„ x0 ç±»åˆ«ç´¢å¼•, shape [M]
    lambda_aux: float = 0.001,     # D3PMè®ºæ–‡å»ºè®®çš„å°å€¼
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆåŸå­ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_x0_indices: çœŸå®çš„ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """
    # 1. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤± (å¯¹åº”äº L_vlb çš„ä¸»è¦éƒ¨åˆ†å’Œ L_aux)
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_x0_indices, reduction='none')
    
    # 2. æ ¹æ® D3PM æ··åˆæŸå¤± L_Î» çš„æ€æƒ³ï¼Œåº”ç”¨æƒé‡
    # L_Î» = L_vlb + Î» * [-log p(x0|xt)]
    # L_vlb çš„ KL é¡¹åœ¨ t>1 æ—¶æƒé‡ä¸º1ï¼Œåœ¨ t=1 æ—¶æƒé‡ä¸º1(é‡å»ºé¡¹)ã€‚
    # L_aux åœ¨æ‰€æœ‰ t ä¸Šæƒé‡éƒ½ä¸º Î»ã€‚
    # æ‰€ä»¥ï¼Œæ€»æƒé‡ä¸º 1 + Î»ï¼Œé™¤äº† t=0 çš„æƒ…å†µï¼ˆæˆ‘ä»¬ä¸å¤„ç†ï¼‰ã€‚
    
    # ä¸€ä¸ªéå¸¸å¸¸è§çš„ç®€åŒ–å®ç°æ˜¯ç›´æ¥åº”ç”¨æƒé‡
    # å¦ä¸€ç§æ¥è‡ªå…¶ä»–è®ºæ–‡çš„æ€è·¯æ˜¯ç»™ä½tçš„æŸå¤±æ›´é«˜çš„æƒé‡
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ L_Î» çš„ç²¾ç¥ï¼šä¸€ä¸ªåŸºç¡€æŸå¤± + ä¸€ä¸ªå°çš„è¾…åŠ©æŸå¤±
    
    # æƒé‡ä¸º 1(æ¥è‡ªL_vlb) + lambda_aux (æ¥è‡ªL_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # å¯¹äº t=1 çš„ç‰¹æ®Šæƒ…å†µï¼ŒVLBä¸­åªæœ‰é‡å»ºé¡¹ï¼Œå¯ä»¥è®¤ä¸ºæƒé‡ä¸åŒ
    # ä½†D3PMçš„æ··åˆæŸå¤±ç®€åŒ–äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¹Ÿé‡‡ç”¨ç®€åŒ–
    
    return final_loss.mean()

# 2.2 åŸå­åæ ‡æŸå¤±
def calculate_coordinate_loss_wrapper(
    predicted_r0: torch.Tensor,      # æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
    true_noise: torch.Tensor,        # ç”¨äºç”Ÿæˆ r_t çš„çœŸå®é«˜æ–¯å™ªå£°, shape [M, 3]
    r_t: torch.Tensor,               # è¾“å…¥åˆ°æ¨¡å‹çš„åŠ å™ªåæ ‡ (t>0), shape [M, 3]
    t: torch.Tensor,                 # æ¯ä¸ªåæ ‡å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]
    scheduler,                       # HierarchicalDiffusionScheduler å®ä¾‹
    schedule_type: str               # ä½¿ç”¨çš„è°ƒåº¦ç±»å‹, 'alpha' æˆ– 'delta'
) -> torch.Tensor:
    """
    è®¡ç®—åŸå­åæ ‡çš„æŸå¤±ã€‚

    è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œå®ƒæ¥æ”¶æ¨¡å‹é¢„æµ‹çš„ r0ï¼Œä½¿ç”¨è°ƒåº¦å™¨å°†å…¶è½¬æ¢ä¸º
    é¢„æµ‹çš„å™ªå£° epsilonï¼Œç„¶åè®¡ç®—ä¸çœŸå®å™ªå£°çš„ L2 æŸå¤±ã€‚

    Args:
        predicted_r0: æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ã€‚
        true_noise: çœŸå®çš„å™ªå£°ã€‚
        r_t: åŠ å™ªåçš„åæ ‡ã€‚
        t: æ—¶é—´æ­¥ã€‚
        scheduler: å™ªå£°è°ƒåº¦å™¨å®ä¾‹ã€‚
        schedule_type: ä½¿ç”¨çš„è°ƒåº¦ç±»å‹ ('alpha' æˆ– 'delta')ã€‚

    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æ ‡é‡æŸå¤±å€¼ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚
    # åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­è¢«æŸç§æ–¹å¼ç§»é™¤äº†ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œè¿™å¯ä»¥é˜²æ­¢å‡ºé”™ã€‚
    if predicted_r0.shape[0] == 0:
        return torch.tensor(0.0, device=predicted_r0.device)

    # 2. è°ƒç”¨è°ƒåº¦å™¨çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä» predicted_r0 åæ¨å‡º predicted_noise
    predicted_noise = scheduler.get_predicted_noise_from_r0(
        r_t=r_t,
        t=t,
        predicted_r0=predicted_r0,
        schedule_type=schedule_type
    )

    # 3. è®¡ç®—é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°ä¹‹é—´çš„ L2 æŸå¤± (å‡æ–¹è¯¯å·®, Mean Squared Error)
    # F.mse_loss(A, B) ä¼šè®¡ç®— (A - B)^2 çš„æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ã€‚
    loss = F.mse_loss(predicted_noise, true_noise)
    
    return loss

# 2.3 è¾¹ç±»å‹æŸå¤±
def calculate_bond_type_loss(
    pred_logits: torch.Tensor,      # æ¨¡å‹å¯¹å¹²å‡€è¾¹ç±»å‹çš„é¢„æµ‹ logits, shape [M_edges, C_bonds]
    true_b0_indices: torch.Tensor, # çœŸå®çš„å¹²å‡€è¾¹ç±»å‹ç´¢å¼•, shape [M_edges]
    lambda_aux: float = 0.001,     # è¾…åŠ©æŸå¤±çš„æƒé‡
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆè¾¹ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ï¼Œä¸åŸå­ç±»å‹æŸå¤±çš„é€»è¾‘å®Œå…¨ç›¸åŒã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_b0_indices: çœŸå®çš„è¾¹ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªè¾¹å¯¹åº”çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ (å½“å‰æœªä½¿ç”¨ï¼Œä¸ºæœªæ¥æ‰©å±•ä¿ç•™)ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚å¦‚æœä¸€ä¸ªæ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦é¢„æµ‹çš„è¾¹ï¼Œåˆ™æŸå¤±ä¸º0ã€‚
    # è¿™åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­æ˜¯å­¤ç«‹ç‚¹æ—¶å¯èƒ½å‘ç”Ÿã€‚
    if pred_logits.shape[0] == 0:
        return torch.tensor(0.0, device=pred_logits.device)
        
    # 2. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯æ¡è¾¹è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_b0_indices, reduction='none')
    
    # 3. åº”ç”¨ D3PM æ··åˆæŸå¤±çš„ç®€åŒ–æƒé‡
    # æ€»æƒé‡ = 1 (æ¥è‡ª L_vlb) + lambda_aux (æ¥è‡ª L_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # 4. å¯¹æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è¾¹çš„æŸå¤±æ±‚å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ ‡é‡æŸå¤±
    return final_loss.mean()


# ==============================================================================
# 3. éªŒè¯å‡½æ•° (Validation Function) - [ä¿®æ”¹åç‰ˆæœ¬]
# ==============================================================================
@torch.no_grad() # è£…é¥°å™¨ï¼Œè¡¨ç¤ºè¯¥å‡½æ•°å†…æ‰€æœ‰ torch è®¡ç®—éƒ½ä¸éœ€è¦è®°å½•æ¢¯åº¦
def validate(val_loader, model, scheduler, args, amp_autocast):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æŸå¤±ã€‚
    æ­¤å‡½æ•°çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—é€»è¾‘ä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€è‡´ã€‚
    """
    device = args.device
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_val_loss = 0.0
    pbar_val = tqdm(val_loader, desc=f"Validating", leave=False)

    for clean_batch in pbar_val:
        clean_batch = clean_batch.to(device)

        with amp_autocast():
            # --- [é€»è¾‘ä¸è®­ç»ƒå¾ªç¯å®Œå…¨ç›¸åŒ] ---

            # --- 0. å‡†å¤‡å·¥ä½œ ---
            num_graphs, num_nodes, num_edges = clean_batch.num_graphs, clean_batch.num_nodes, clean_batch.num_edges
            scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
            t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device)
            t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
            noise1, noise2 = torch.randn_like(scaled_pos), torch.randn_like(scaled_pos)
            t1_per_node, t1_per_edge = t1[clean_batch.batch], t1[clean_batch.batch[clean_batch.edge_index[0]]]

            # --- ç­–ç•¥ I: å…¨å±€å»å™ª ---
            noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, 'alpha')
            noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            noised_data_I = clean_batch.clone(); noised_data_I.pos, noised_data_I.x, noised_data_I.edge_attr = noised_pos_I, noised_x_I, noised_edge_attr_I
            
            target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
            target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)
            
            predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)
            
            lossI_a = calculate_atom_type_loss(predictions_I['atom_type_logits'], clean_batch.x.argmax(dim=-1), t1_per_node, args.lambda_aux, scheduler.T_full)
            lossI_r = calculate_coordinate_loss_wrapper(predictions_I['predicted_r0'], noise1, noised_pos_I, t1_per_node, scheduler, 'alpha')
            lossI_b = calculate_bond_type_loss(predictions_I['bond_logits'], clean_batch.edge_attr.argmax(dim=-1), t1_per_edge, args.lambda_aux, scheduler.T_full)
            loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b
    
            # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ ---
            target_node_mask_II = clean_batch.is_new_node.squeeze().bool()
            context_node_mask_II = ~target_node_mask_II
            target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
            context_edge_mask = ~target_edge_mask
            
            t_T1_per_node, t_T1_per_edge = torch.full_like(t1_per_node, scheduler.T1), torch.full_like(t1_per_edge, scheduler.T1)
            t2_per_node, t2_per_edge = t2[clean_batch.batch], t2[clean_batch.batch[clean_batch.edge_index[0]]]
    
            noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
            noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
            noised_pos_II = torch.zeros_like(scaled_pos); noised_pos_II[context_node_mask_II], noised_pos_II[target_node_mask_II] = noised_pos_context, noised_pos_target
            
            noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
            noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
            noised_x_II = torch.zeros_like(clean_batch.x); noised_x_II[context_node_mask_II], noised_x_II[target_node_mask_II] = noised_x_context, noised_x_target
        
            noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
            noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
            noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr); noised_edge_attr_II[context_edge_mask], noised_edge_attr_II[target_edge_mask] = noised_edge_attr_context, noised_edge_attr_target
        
            noised_data_II = clean_batch.clone(); noised_data_II.pos, noised_data_II.x, noised_data_II.edge_attr = noised_pos_II, noised_x_II, noised_edge_attr_II
        
            predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

            lossII_a = calculate_atom_type_loss(predictions_II['atom_type_logits'], clean_batch.x[target_node_mask_II].argmax(dim=-1), t2_per_node[target_node_mask_II], args.lambda_aux, scheduler.T_full)
            lossII_r = calculate_coordinate_loss_wrapper(predictions_II['predicted_r0'], noise2[target_node_mask_II], noised_pos_target, t2_per_node[target_node_mask_II], scheduler, 'delta')
            lossII_b = calculate_bond_type_loss(predictions_II['bond_logits'], clean_batch.edge_attr[target_edge_mask].argmax(dim=-1), t2_per_edge[target_edge_mask], args.lambda_aux, scheduler.T_full)
            loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b

            # --- æ€»éªŒè¯æŸå¤± ---
            total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II 
        
        total_val_loss += total_loss.item()
        pbar_val.set_postfix({'val_loss': total_loss.item()})
    
    avg_val_loss = total_val_loss / len(val_loader)
    return avg_val_loss

# ==============================================================================
# 4. ä¸»è®­ç»ƒå‡½æ•° (Main Training Function)
# ==============================================================================

def train(
    args, # åŒ…å«è¶…å‚æ•°çš„å¯¹è±¡ï¼Œå¦‚å­¦ä¹ ç‡ã€epochæ•°ç­‰
    logger,
    train_loader, # è®­ç»ƒæ•°æ®
    val_loader, # æµ‹è¯•æ•°æ®
    model: nn.Module, # E_DiT_Network å®ä¾‹
    scheduler, # HierarchicalDiffusionScheduler å®ä¾‹
    amp_autocast,
    loss_scaler
):
    """
    ä¸»è®­ç»ƒå‡½æ•°ã€‚
    """
    device = args.device
    model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    T_max = args.epochs  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸ºæ€» epoch æ•°
    lr_min_factor = args.lr_min_factor
    scheduler_model = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=lr_min_factor * args.learning_rate)

    best_val_loss = float('inf')
    best_epoch = 0
    
    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨: {args.checkpoints_dir}")
    logger.info("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(1, args.epochs + 1):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºâ€œè®­ç»ƒæ¨¡å¼â€
        # å®ƒä¼šé€šçŸ¥æ¨¡å‹ä¸­æ‰€æœ‰å…·æœ‰ä¸åŒè®­ç»ƒ/è¯„ä¼°è¡Œä¸ºçš„å±‚ï¼ˆä¸»è¦æ˜¯ Dropout å±‚å’Œ BatchNorm å±‚ï¼‰åˆ‡æ¢åˆ°å®ƒä»¬çš„è®­ç»ƒçŠ¶æ€ã€‚
        # Dropout å±‚åœ¨è®­ç»ƒæ—¶ä¼šéšæœºâ€œä¸¢å¼ƒâ€ä¸€äº›ç¥ç»å…ƒï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¸ä¼šä¸¢å¼ƒï¼Œä¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒã€‚
        # BatchNorm å±‚åœ¨è®­ç»ƒæ—¶ä¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ›´æ–°å…¶å†…éƒ¨çš„å…¨å±€ç»Ÿè®¡é‡ï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¼šä½¿ç”¨å·²å­¦ä¹ åˆ°çš„å…¨å±€ç»Ÿè®¡é‡ã€‚
        model.train()  

        # åˆå§‹åŒ–ä¸€ä¸ªå˜é‡ï¼Œç”¨äºç´¯åŠ å½“å‰è¿™ä¸ª epoch å†…æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±å€¼
        # åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ total_loss_epoch é™¤ä»¥æ‰¹æ¬¡çš„æ€»æ•°ï¼Œæ¥è®¡ç®—å¹¶æ‰“å°å‡ºè¿™ä¸ª epoch çš„å¹³å‡æŸå¤±ï¼Œä»¥æ­¤æ¥ç›‘æ§è®­ç»ƒçš„è¿›å±•ã€‚
        total_loss_epoch = 0.0
        
        # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{args.epochs}")  # desc=...: è®¾ç½®è¿›åº¦æ¡å·¦ä¾§çš„æè¿°æ€§æ–‡å­—ï¼Œä¾‹å¦‚ Epoch 1/100
        
        for clean_batch in pbar:
            clean_batch = clean_batch.to(device) # å°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¼ é‡ï¼Œå¦‚ pos, x, edge_index ç­‰ï¼‰ä¸€æ¬¡æ€§åœ°ç§»åŠ¨åˆ°ä¹‹å‰å®šä¹‰å¥½çš„ç›®æ ‡è®¾å¤‡ device ä¸Š
            # æ¸…é™¤ä¼˜åŒ–å™¨ä¸­æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼ŒPyTorch çš„æ¢¯åº¦æ˜¯ç´¯åŠ çš„ã€‚åœ¨æ¯æ¬¡è®¡ç®—æ–°ä¸€æ‰¹æ•°æ®çš„æ¢¯åº¦å¹¶è¿›è¡Œåå‘ä¼ æ’­ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨åœ°å°†ä¸Šä¸€æ¬¡è®¡ç®—çš„æ¢¯åº¦æ¸…é›¶ã€‚
            # å¦åˆ™ï¼Œæ–°çš„æ¢¯åº¦ä¼šç´¯åŠ åˆ°æ—§çš„æ¢¯åº¦ä¸Šï¼Œå¯¼è‡´é”™è¯¯çš„å‚æ•°æ›´æ–°æ–¹å‘ã€‚è¿™æ˜¯ PyTorch è®­ç»ƒå¾ªç¯ä¸­ä¸€ä¸ªå¿…ä¸å¯å°‘çš„æ­¥éª¤ã€‚
            optimizer.zero_grad() 

            with amp_autocast():
                # --- 0. å‡†å¤‡å·¥ä½œ ---
                num_graphs = clean_batch.num_graphs # æ‰¹æ¬¡ä¸­åŒ…å«çš„ç‹¬ç«‹å›¾çš„æ•°é‡ï¼ˆç­‰äº batch_sizeï¼‰ã€‚ç”¨äºé‡‡æ ·å›¾çº§åˆ«çš„å˜é‡ï¼Œå¦‚æ—¶é—´æ­¥ t
                num_nodes = clean_batch.num_nodes #  æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„èŠ‚ç‚¹æ€»æ•°
                num_edges = clean_batch.num_edges # æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„è¾¹æ€»æ•°

                # a. åæ ‡ç¼©æ”¾
                scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
            
                # b. é‡‡æ ·æ—¶é—´æ­¥å’Œé«˜æ–¯å™ªå£°
                # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªå›¾éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ t1
                # t1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [batch_size] çš„å¼ é‡ï¼Œä¾‹å¦‚ tensor([18, 98, 21, ...])
                t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device) 
                t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
                # noise1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, 3] çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ã€‚noise1[i] å°±æ˜¯è¦åŠ åˆ°ç¬¬ i ä¸ªåŸå­åæ ‡ä¸Šçš„å™ªå£°å‘é‡ã€‚
                noise1 = torch.randn_like(scaled_pos)
                noise2 = torch.randn_like(scaled_pos)
            
                # c. å°† per-graph çš„æ—¶é—´æ­¥æ‰©å±•åˆ° per-node å’Œ per-edge
                # t1: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_graphs] çš„å¼ é‡ã€‚å‡è®¾ batch_size=4ï¼Œt1å¯èƒ½é•¿è¿™æ ·ï¼štensor([18, 98, 21, 76])
                # clean_batch.batch: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_nodes] çš„å¼ é‡ï¼Œè®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªä¸ªå›¾ã€‚
                # å®ƒå¯èƒ½é•¿è¿™æ ·ï¼ˆå‡è®¾4ä¸ªå›¾åˆ†åˆ«æœ‰3, 2, 4, 3ä¸ªèŠ‚ç‚¹ï¼‰ï¼štensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3])ã€‚
                t1_per_node = t1[clean_batch.batch] # å½¢çŠ¶ä¸º[num_nodes]
                t1_per_edge = t1[clean_batch.batch[clean_batch.edge_index[0]]] # å½¢çŠ¶ä¸º[num_edges]


                # --- ç­–ç•¥ I: å…¨å±€å»å™ª (ç”Ÿæˆå™ªå£°å›¾ â… ) ---
            
                # a. åŠ å™ªåæ ‡
                noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, schedule_type='alpha')  # é˜…è¯»æ ‡è®°
            
                # b. åŠ å™ªåŸå­ç±»å‹
                noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            
                # c. åŠ å™ªè¾¹å±æ€§
                noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â… 
                # å¤åˆ¶å¹²å‡€çš„æ•°æ®ï¼Œæ›´æ”¹åŠ å™ªçš„éƒ¨åˆ†
                noised_data_I = clean_batch.clone()
                noised_data_I.pos = noised_pos_I
                noised_data_I.x = noised_x_I
                noised_data_I.edge_attr = noised_edge_attr_I
            
                # e. å‡†å¤‡æ¨¡å‹è¾“å…¥
                # åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºå½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰åŸå­çš„æ€»æ•°ï¼Œå†…å®¹å…¨ä¸º True çš„å‘é‡ã€‚
                target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
                # å¤„ç†å¹¶è¾“å‡ºæ‰€æœ‰è¾¹çš„é¢„æµ‹ç»“æœ
                target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)

                # f. æ¨¡å‹å‰å‘ä¼ æ’­
                predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)
            
                # g. è®¡ç®—æŸå¤± â… 
                lossI_a = calculate_atom_type_loss(
                    predictions_I['atom_type_logits'],
                    clean_batch.x.argmax(dim=-1),  # ä» One-Hot ç¼–ç çš„ç‰¹å¾å¼ é‡ä¸­ï¼Œæå–å‡ºæ¯ä¸ªé¡¹ç›®å¯¹åº”çš„ç±»åˆ«ç´¢å¼• (class index)
                    args.lambda_aux
                )
                lossI_r = calculate_coordinate_loss_wrapper(
                    predicted_r0=predictions_I['predicted_r0'], 
                    true_noise=noise1, 
                    r_t=noised_pos_I, 
                    t=t1_per_node, 
                    scheduler=scheduler, 
                    schedule_type='alpha'
                )
                lossI_b = calculate_bond_type_loss(
                    pred_logits=predictions_I['bond_logits'], 
                    true_b0_indices=clean_batch.edge_attr.argmax(dim=-1),
                    lambda_aux=args.lambda_aux
                )
                loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b


                # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ (ç”Ÿæˆå™ªå£°å›¾ â…¡) ---

                # a. è¯†åˆ«ä¸Šä¸‹æ–‡å’Œç›®æ ‡
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç›®æ ‡
                target_node_mask_II = clean_batch.is_new_node.squeeze() # is_new_node å°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡maskï¼Œç»´åº¦å‹ç¼©ä¸º[num_nodes]
                target_node_mask_II = target_node_mask_II.to(torch.bool)
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯ä¸Šä¸‹æ–‡èŠ‚ç‚¹ï¼Œç”¨äºå¯¹ä¸Šä¸‹æ–‡èŠ‚ç‚¹åŠ å™ª
                context_node_mask_II = ~target_node_mask_II
                # æ ‡è¯†å“ªäº›è¾¹æ˜¯ä¸é¢„æµ‹ç›®æ ‡èŠ‚ç‚¹ç›¸å…³çš„è¾¹
                # å¯¹äºç¬¬ i æ¡è¾¹ï¼Œå¦‚æœå®ƒçš„èµ·ç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹æˆ–è€…å®ƒçš„ç»ˆç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯éœ€è¦è¢«é¢„æµ‹çš„è¾¹
                target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
                # ç”¨äºå¯¹ä¸Šä¸‹æ–‡è¾¹åŠ å™ª
                context_edge_mask = ~target_edge_mask

                # b. å‡†å¤‡æ—¶é—´æ­¥ (T1 å’Œ t2)
                # åˆ›å»ºä¸€ä¸ªä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒã€ç±»å‹ç›¸åŒã€è®¾å¤‡ç›¸åŒçš„æ–°å¼ é‡ï¼Œå¹¶å°†æ‰€æœ‰å…ƒç´ å¡«å……ä¸ºT1
                t_T1_per_node = torch.full_like(t1_per_node, fill_value=scheduler.T1)
                t_T1_per_edge = torch.full_like(t1_per_edge, fill_value=scheduler.T1)
                t2_per_node = t2[clean_batch.batch]
                t2_per_edge = t2[clean_batch.batch[clean_batch.edge_index[0]]]

                # c. å¯¹ä¸Šä¸‹æ–‡å’Œç›®æ ‡åˆ†åˆ«åŠ å™ª
                # åæ ‡
                # è®¡ç®—å‡ºæ‰€æœ‰ä¸Šä¸‹æ–‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
                # è®¡ç®—å‡ºæ‰€æœ‰ç›®æ ‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
                # åˆ›å»ºä¸€ä¸ªç©ºçš„â€œç”»å¸ƒâ€
                noised_pos_II = torch.zeros_like(scaled_pos)
                # å°†è®¡ç®—å¥½çš„ä¸Šä¸‹æ–‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[context_node_mask_II] = noised_pos_context
                # å°†è®¡ç®—å¥½çš„ç›®æ ‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[target_node_mask_II] = noised_pos_target

                # åŸå­ç±»å‹
                noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
                noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
                noised_x_II = torch.zeros_like(clean_batch.x)
                noised_x_II[context_node_mask_II] = noised_x_context
                noised_x_II[target_node_mask_II] = noised_x_target
            
                # è¾¹å±æ€§
                noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
                noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
                noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr)
                noised_edge_attr_II[context_edge_mask] = noised_edge_attr_context
                noised_edge_attr_II[target_edge_mask] = noised_edge_attr_target
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â…¡
                noised_data_II = clean_batch.clone()
                noised_data_II.pos = noised_pos_II
                noised_data_II.x = noised_x_II
                noised_data_II.edge_attr = noised_edge_attr_II

            
                # f. æ¨¡å‹å‰å‘ä¼ æ’­ (æ³¨æ„æ—¶é—´æ­¥ä¼ å…¥çš„æ˜¯ t2)
                predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

                # g. è®¡ç®—æŸå¤± â…¡
                # æ³¨æ„ï¼šè¿™é‡Œçš„çœŸå®æ ‡ç­¾å’Œå™ªå£°éƒ½éœ€è¦æ ¹æ® mask è¿›è¡Œç­›é€‰
                lossII_a = calculate_atom_type_loss(
                    predictions_II['atom_type_logits'],
                    clean_batch.x[target_node_mask_II].argmax(dim=-1),
                    args.lambda_aux
                )
                lossII_r = calculate_coordinate_loss_wrapper(
                    predicted_r0=predictions_II['predicted_r0'], 
                    true_noise=noise2[target_node_mask_II], 
                    r_t=noised_pos_target, 
                    t=t2_per_node[target_node_mask_II], 
                    scheduler=scheduler, 
                    schedule_type='delta'
                )
                lossII_b = calculate_bond_type_loss(
                    pred_logits=predictions_II['bond_logits'],
                    true_b0_indices=clean_batch.edge_attr[target_edge_mask].argmax(dim=-1),
                    lambda_aux=args.lambda_aux
                )
                loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b


                # --- æ€»æŸå¤±ä¸åå‘ä¼ æ’­ ---
                total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II

            # ç¼©æ”¾ç”Ÿæˆæ¨¡å‹çš„æŸå¤±å¹¶è®¡ç®—æ¢¯åº¦
            loss_scaler(
                loss=total_loss,
                optimizer=optimizer,
                parameters=model.parameters()
            )
            
            total_loss_epoch += total_loss.item()
            pbar.set_postfix({'loss': total_loss.item()})
            
        avg_train_loss = total_loss_epoch / len(train_loader)
        logger.info(f"Epoch {epoch} [Train] å®Œæˆ, å¹³å‡æŸå¤±: {avg_train_loss:.4f}")


        # --- éªŒè¯é˜¶æ®µ ---
        if epoch >= args.val_thre and (epoch % args.val_log_freq == 0):
            avg_val_loss = validate(val_loader, model, scheduler, args, amp_autocast)
            logger.info(f"Epoch {epoch} [Validation] å®Œæˆ, å¹³å‡æŸå¤±: {avg_val_loss:.4f}")

            # ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹ 
            logger.info(f"åœ¨ Epoch {epoch} ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹åŠå…¶éªŒè¯æŸå¤±...")

            checkpoint_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_model_state_dict': optimizer.state_dict(),
                'scheduler_model_state_dict': scheduler_model.state_dict(),    
                'loss_scaler_state_dict': loss_scaler.state_dict(),
                'validation_loss': avg_val_loss, # <-- æ˜ç¡®ä¿å­˜å½“å‰ epoch çš„éªŒè¯æŸå¤±
                'args': args
            }
            # ä½¿ç”¨åŒ…å« epoch ç¼–å·çš„å”¯ä¸€æ–‡ä»¶å
            checkpoint_path = os.path.join(args.checkpoints_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint_state, checkpoint_path)

            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_epoch = epoch
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜æœ€ä½³æ¨¡å‹...")
                
                # ä¸ºæœ€ä½³æ¨¡å‹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ä¿å­˜çŠ¶æ€
                best_model_state = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'best_val_loss': best_val_loss,
                    'args': args
                }
                
                best_model_path = os.path.join(args.checkpoints_dir, 'best_model.pth')
                torch.save(best_model_state, best_model_path)
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_model.step()
            
    logger.info("è®­ç»ƒå®Œæˆã€‚")
    logger.info(f"æœ€ç»ˆï¼Œæœ€ä½³æ¨¡å‹å‘ç°åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º: {best_val_loss:.4f}")