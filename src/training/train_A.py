import torch
import torch.optim as optim
from tqdm import tqdm
import torch.nn.functional as F
import os
# from torch.optim.lr_scheduler import ReduceLROnPlateau # <-- æ–°å¢: å¯¼å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import CosineAnnealingLR  # å¯¼å…¥ CosineAnnealingLR
from torch_geometric.utils import scatter

# å‡è®¾æ¨¡å‹å’Œè°ƒåº¦å™¨å·²ç»å®šä¹‰å¥½
# from e_dit_network import E_DiT_Network
# from scheduler import HierarchicalDiffusionScheduler

# ==============================================================================
# 1. è¾…åŠ©å‡½æ•° (Helper Functions)
# ==============================================================================

def scale_to_unit_sphere(pos: torch.Tensor, batch_map: torch.Tensor) -> torch.Tensor:
    """
    å°†æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾çš„åæ ‡ç‹¬ç«‹åœ°ç¼©æ”¾åˆ°å•ä½çƒå†…ã€‚
    æ­¤ç‰ˆæœ¬ç¡®ä¿äº†è®¾å¤‡ä¸€è‡´æ€§ã€‚

    Args:
        pos (torch.Tensor): æ‰¹æ¬¡ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åæ ‡å¼ é‡, shape [N, 3]ã€‚
        batch_map (torch.Tensor): å°†æ¯ä¸ªèŠ‚ç‚¹æ˜ å°„åˆ°å…¶æ‰€å±å›¾çš„å‘é‡, shape [N]ã€‚

    Returns:
        torch.Tensor: ç¼©æ”¾åçš„åæ ‡ï¼Œä¸è¾“å…¥å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Šã€‚
    """
    # PyG çš„ scatter å‡½æ•°å¯ä»¥é«˜æ•ˆåœ°æŒ‰ç»„æ±‚å’Œ/æ±‚å‡å€¼

    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åˆ°å…¶è´¨å¿ƒçš„è·ç¦»
    distances = torch.linalg.norm(pos, dim=1)

    # æŒ‰å›¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå›¾ä¸­çš„æœ€å¤§è·ç¦»
    # scatter å‡½æ•°ä¼šä¿æŒè®¾å¤‡ä¸€è‡´æ€§
    max_distances = scatter(distances, batch_map, dim=0, reduce='max')

    # è®¡ç®—æ¯ä¸ªå›¾çš„ç¼©æ”¾å› å­ï¼ŒåŠ ä¸Šä¸€ä¸ªå°çš„ epsilon é˜²æ­¢é™¤ä»¥é›¶
    # ç´¢å¼•æ“ä½œåŒæ ·ä¼šä¿æŒè®¾å¤‡ä¸€è‡´æ€§
    scale_factors = max_distances[batch_map].unsqueeze(1) + 1e-8

    # ç¼©æ”¾åæ ‡ï¼Œæœ€ç»ˆç»“æœä¸è¾“å…¥ pos åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
    return pos / scale_factors


def noise_discrete_features(
    features_0: torch.Tensor,
    Q_bar: torch.Tensor,
    t_per_item: torch.Tensor
) -> torch.Tensor:
    """
    å¯¹ one-hot ç¼–ç çš„ç¦»æ•£ç‰¹å¾ï¼ˆå¦‚åŸå­ç±»å‹ã€è¾¹ç±»å‹ï¼‰è¿›è¡ŒåŠ å™ªã€‚

    Args:
        features_0 (torch.Tensor): å¹²å‡€çš„ one-hot ç‰¹å¾, shape [M, K] (Mä¸ªé¡¹ç›®, Kä¸ªç±»åˆ«)ã€‚
        Q_bar (torch.Tensor):      è½¬ç§»çŸ©é˜µé›†åˆ, shape [T, K, K] (Tä¸ªæ—¶é—´æ­¥)ã€‚
        t_per_item (torch.Tensor): æ¯ä¸ªé¡¹ç›®å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]ã€‚

    Returns:
        torch.Tensor: åŠ å™ªåçš„ one-hot ç‰¹å¾ã€‚
    """
    # 1. æ ¹æ®æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ tï¼Œä» Q_bar ä¸­é€‰å‡ºå¯¹åº”çš„è½¬ç§»çŸ©é˜µ
    # Q_bar_t çš„ shape ä¸º [M, K, K]
    Q_bar_t = Q_bar[t_per_item]
    Q_bar_t = Q_bar_t.to(features_0.device)
    
    # 2. è®¡ç®—åŠ å™ªåçš„æ¦‚ç‡åˆ†å¸ƒ
    # features_0.unsqueeze(1) -> [M, 1, K]   åœ¨ features_0 çš„ç¬¬1ä¸ªç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
    # Q_bar_t                 -> [M, K, K]
    # prob_t                  -> [M, 1, K]
    # æ‰§è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
    prob_t = torch.bmm(features_0.unsqueeze(1), Q_bar_t).squeeze(1) # shape: [M, K]
    
    # 3. æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°æ–°çš„ç±»åˆ«ç´¢å¼•
    # torch.multinomial è¦æ±‚è¾“å…¥æ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬è¿™é‡Œå·²ç»æ˜¯æ¦‚ç‡äº†
    # multinomial ä¼šæŠŠæ¯ä¸€è¡Œéƒ½çœ‹ä½œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„â€œéª°å­â€çš„æ¦‚ç‡è®¾ç½®
    # num_samples=1 æŒ‡æ¯è¡Œé‡‡æ ·ä¸€æ¬¡ï¼Œè¿™æ—¶è¾“å‡ºç»´åº¦ä¸º[M, 1]
    # .squeeze(-1) ä½œç”¨ä¸ºç§»é™¤æœ€åä¸€ä¸ªç»´åº¦ï¼ˆdim=-1ï¼‰ä¸Šå¤§å°ä¸º1çš„ç»´åº¦
    sampled_indices = torch.multinomial(prob_t, num_samples=1).squeeze(-1) # shape: [M]
    
    # 4. å°†é‡‡æ ·å‡ºçš„ç´¢å¼•è½¬æ¢å› one-hot ç¼–ç 
    num_classes = features_0.shape[1]  # è·å–ç±»åˆ«çš„æ€»æ•°
    # å°†æ•´æ•°ç±»åˆ«ç´¢å¼•è½¬æ¢æˆ One-Hot ç¼–ç å‘é‡
    features_t = torch.nn.functional.one_hot(sampled_indices, num_classes=num_classes).float()
    
    return features_t

# ==============================================================================
# 2. æŸå¤±å‡½æ•°æ¡†æ¶ (Loss Function Skeletons)
# ==============================================================================

# 2.1 åŸå­ç±»å‹æŸå¤±
# æ¨èä½¿ç”¨çš„ã€æ›´ç®€æ´çš„æŸå¤±å‡½æ•°
def calculate_atom_type_loss(
    pred_logits: torch.Tensor,   # æ¨¡å‹å¯¹ x0 çš„é¢„æµ‹ logits, shape [M, C]
    true_x0_indices: torch.Tensor, # çœŸå®çš„ x0 ç±»åˆ«ç´¢å¼•, shape [M]
    lambda_aux: float = 0.001,     # D3PMè®ºæ–‡å»ºè®®çš„å°å€¼
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆåŸå­ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_x0_indices: çœŸå®çš„ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """

    ### --- ä¿®æ­£ï¼šä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼Œå°† logits è½¬æ¢ä¸º float32 --- ###
    # F.cross_entropy åœ¨ float16 ä¸‹å¯èƒ½ä¸ç¨³å®š
    stable_logits = pred_logits.to(torch.float32)

    # 1. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤± (å¯¹åº”äº L_vlb çš„ä¸»è¦éƒ¨åˆ†å’Œ L_aux)
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(stable_logits, true_x0_indices, reduction='none')
    
    # 2. æ ¹æ® D3PM æ··åˆæŸå¤± L_Î» çš„æ€æƒ³ï¼Œåº”ç”¨æƒé‡
    # L_Î» = L_vlb + Î» * [-log p(x0|xt)]
    # L_vlb çš„ KL é¡¹åœ¨ t>1 æ—¶æƒé‡ä¸º1ï¼Œåœ¨ t=1 æ—¶æƒé‡ä¸º1(é‡å»ºé¡¹)ã€‚
    # L_aux åœ¨æ‰€æœ‰ t ä¸Šæƒé‡éƒ½ä¸º Î»ã€‚
    # æ‰€ä»¥ï¼Œæ€»æƒé‡ä¸º 1 + Î»ï¼Œé™¤äº† t=0 çš„æƒ…å†µï¼ˆæˆ‘ä»¬ä¸å¤„ç†ï¼‰ã€‚
    
    # ä¸€ä¸ªéå¸¸å¸¸è§çš„ç®€åŒ–å®ç°æ˜¯ç›´æ¥åº”ç”¨æƒé‡
    # å¦ä¸€ç§æ¥è‡ªå…¶ä»–è®ºæ–‡çš„æ€è·¯æ˜¯ç»™ä½tçš„æŸå¤±æ›´é«˜çš„æƒé‡
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ L_Î» çš„ç²¾ç¥ï¼šä¸€ä¸ªåŸºç¡€æŸå¤± + ä¸€ä¸ªå°çš„è¾…åŠ©æŸå¤±
    
    # æƒé‡ä¸º 1(æ¥è‡ªL_vlb) + lambda_aux (æ¥è‡ªL_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # å¯¹äº t=1 çš„ç‰¹æ®Šæƒ…å†µï¼ŒVLBä¸­åªæœ‰é‡å»ºé¡¹ï¼Œå¯ä»¥è®¤ä¸ºæƒé‡ä¸åŒ
    # ä½†D3PMçš„æ··åˆæŸå¤±ç®€åŒ–äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¹Ÿé‡‡ç”¨ç®€åŒ–
    
    return final_loss.mean()

# 2.2 åŸå­åæ ‡æŸå¤±
def calculate_coordinate_loss_wrapper(
    predicted_r0: torch.Tensor,      # æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
    true_noise: torch.Tensor,        # ç”¨äºç”Ÿæˆ r_t çš„çœŸå®é«˜æ–¯å™ªå£°, shape [M, 3]
    r_t: torch.Tensor,               # è¾“å…¥åˆ°æ¨¡å‹çš„åŠ å™ªåæ ‡ (t>0), shape [M, 3]
    t: torch.Tensor,                 # æ¯ä¸ªåæ ‡å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]
    scheduler,                       # HierarchicalDiffusionScheduler å®ä¾‹
    schedule_type: str               # ä½¿ç”¨çš„è°ƒåº¦ç±»å‹, 'alpha' æˆ– 'delta'
) -> torch.Tensor:
    """
    è®¡ç®—åŸå­åæ ‡çš„æŸå¤±ã€‚

    è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œå®ƒæ¥æ”¶æ¨¡å‹é¢„æµ‹çš„ r0ï¼Œä½¿ç”¨è°ƒåº¦å™¨å°†å…¶è½¬æ¢ä¸º
    é¢„æµ‹çš„å™ªå£° epsilonï¼Œç„¶åè®¡ç®—ä¸çœŸå®å™ªå£°çš„ L2 æŸå¤±ã€‚

    Args:
        predicted_r0: æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ã€‚
        true_noise: çœŸå®çš„å™ªå£°ã€‚
        r_t: åŠ å™ªåçš„åæ ‡ã€‚
        t: æ—¶é—´æ­¥ã€‚
        scheduler: å™ªå£°è°ƒåº¦å™¨å®ä¾‹ã€‚
        schedule_type: ä½¿ç”¨çš„è°ƒåº¦ç±»å‹ ('alpha' æˆ– 'delta')ã€‚

    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æ ‡é‡æŸå¤±å€¼ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚
    # åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­è¢«æŸç§æ–¹å¼ç§»é™¤äº†ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œè¿™å¯ä»¥é˜²æ­¢å‡ºé”™ã€‚
    if predicted_r0.shape[0] == 0:
        return torch.tensor(0.0, device=predicted_r0.device)

    # 2. è°ƒç”¨è°ƒåº¦å™¨çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä» predicted_r0 åæ¨å‡º predicted_noise
    predicted_noise = scheduler.get_predicted_noise_from_r0(
        r_t=r_t,
        t=t,
        predicted_r0=predicted_r0,
        schedule_type=schedule_type
    )

    # 3. è®¡ç®—é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°ä¹‹é—´çš„ L2 æŸå¤± (å‡æ–¹è¯¯å·®, Mean Squared Error)
    # F.mse_loss(A, B) ä¼šè®¡ç®— (A - B)^2 çš„æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ã€‚
    loss = F.mse_loss(predicted_noise, true_noise)
    
    return loss

# 2.3 è¾¹ç±»å‹æŸå¤±
def calculate_bond_type_loss(
    pred_logits: torch.Tensor,      # æ¨¡å‹å¯¹å¹²å‡€è¾¹ç±»å‹çš„é¢„æµ‹ logits, shape [M_edges, C_bonds]
    true_b0_indices: torch.Tensor, # çœŸå®çš„å¹²å‡€è¾¹ç±»å‹ç´¢å¼•, shape [M_edges]
    lambda_aux: float = 0.001,     # è¾…åŠ©æŸå¤±çš„æƒé‡
) -> torch.Tensor:

    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆè¾¹ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ï¼Œä¸åŸå­ç±»å‹æŸå¤±çš„é€»è¾‘å®Œå…¨ç›¸åŒã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_b0_indices: çœŸå®çš„è¾¹ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªè¾¹å¯¹åº”çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ (å½“å‰æœªä½¿ç”¨ï¼Œä¸ºæœªæ¥æ‰©å±•ä¿ç•™)ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """

    print("--- DEBUG: Bond Logits ---")
    print(f"Shape: {pred_logits.shape}, Dtype: {pred_logits.dtype}")
    if pred_logits.numel() > 0:
        print(f"Min: {pred_logits.min().item()}, Max: {pred_logits.max().item()}, Mean: {pred_logits.mean().item()}")
    print(f"Contains NaN: {torch.isnan(pred_logits).any().item()}")
    print(f"Contains Inf: {torch.isinf(pred_logits).any().item()}")
    print("--------------------------")
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚å¦‚æœä¸€ä¸ªæ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦é¢„æµ‹çš„è¾¹ï¼Œåˆ™æŸå¤±ä¸º0ã€‚
    # è¿™åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­æ˜¯å­¤ç«‹ç‚¹æ—¶å¯èƒ½å‘ç”Ÿã€‚
    if pred_logits.shape[0] == 0:
        return torch.tensor(0.0, device=pred_logits.device)

    stable_logits = pred_logits.to(torch.float32)
        
    # 2. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯æ¡è¾¹è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(stable_logits, true_b0_indices, reduction='none')
    
    # 3. åº”ç”¨ D3PM æ··åˆæŸå¤±çš„ç®€åŒ–æƒé‡
    # æ€»æƒé‡ = 1 (æ¥è‡ª L_vlb) + lambda_aux (æ¥è‡ª L_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # 4. å¯¹æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è¾¹çš„æŸå¤±æ±‚å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ ‡é‡æŸå¤±
    return final_loss.mean()


# ==============================================================================
# 3. éªŒè¯å‡½æ•° (Validation Function)
# ==============================================================================
@torch.no_grad() # è£…é¥°å™¨ï¼Œè¡¨ç¤ºè¯¥å‡½æ•°å†…æ‰€æœ‰ torch è®¡ç®—éƒ½ä¸éœ€è¦è®°å½•æ¢¯åº¦
def validate(val_loader, model, s_model, scheduler, args, amp_autocast, fragmenter):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æŸå¤±ã€‚
    æ­¤å‡½æ•°çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—é€»è¾‘ä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€è‡´ã€‚
    """
    device = args.device
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_val_loss = 0.0
    total_s_val_loss_epoch = 0.0
    pbar_val = tqdm(val_loader, desc=f"Validating", leave=False)

    for batch_non_fc, batch_fc in pbar_val:
        batch_fc = batch_fc.to(device)
        batch_non_fc = batch_non_fc.to(device)

        with amp_autocast():
            # å°†éå…¨è¿æ¥çš„ batch é€å…¥æ’åºç½‘ç»œ
            orders, log_prob_orders = s_model(batch_non_fc)

            clean_batch = seg(batch_fc, orders, fragmenter)

            # --- [é€»è¾‘ä¸è®­ç»ƒå¾ªç¯å®Œå…¨ç›¸åŒ] ---

            # --- 0. å‡†å¤‡å·¥ä½œ ---
            num_graphs, num_nodes, num_edges = clean_batch.num_graphs, clean_batch.num_nodes, clean_batch.num_edges
            scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
            t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device)
            t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
            noise1, noise2 = torch.randn_like(scaled_pos), torch.randn_like(scaled_pos)
            t1_per_node, t1_per_edge = t1[clean_batch.batch], t1[clean_batch.batch[clean_batch.edge_index[0]]]

            # --- ç­–ç•¥ I: å…¨å±€å»å™ª ---
            noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, 'alpha')
            noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            noised_data_I = clean_batch.clone(); noised_data_I.pos, noised_data_I.x, noised_data_I.edge_attr = noised_pos_I, noised_x_I, noised_edge_attr_I
            
            target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
            target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)
            
            predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)
            
            lossI_a = calculate_atom_type_loss(predictions_I['atom_type_logits'], clean_batch.x.argmax(dim=-1), t1_per_node, args.lambda_aux, scheduler.T_full)
            lossI_r = calculate_coordinate_loss_wrapper(predictions_I['predicted_r0'], noise1, noised_pos_I, t1_per_node, scheduler, 'alpha')
            lossI_b = calculate_bond_type_loss(predictions_I['bond_logits'], clean_batch.edge_attr.argmax(dim=-1), t1_per_edge, args.lambda_aux, scheduler.T_full)
            loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b
    
            # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ ---
            target_node_mask_II = clean_batch.is_new_node.squeeze().bool()
            context_node_mask_II = ~target_node_mask_II
            target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
            context_edge_mask = ~target_edge_mask
            
            t_T1_per_node, t_T1_per_edge = torch.full_like(t1_per_node, scheduler.T1), torch.full_like(t1_per_edge, scheduler.T1)
            t2_per_node, t2_per_edge = t2[clean_batch.batch], t2[clean_batch.batch[clean_batch.edge_index[0]]]
    
            noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
            noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
            noised_pos_II = torch.zeros_like(scaled_pos); noised_pos_II[context_node_mask_II], noised_pos_II[target_node_mask_II] = noised_pos_context, noised_pos_target
            
            noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
            noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
            noised_x_II = torch.zeros_like(clean_batch.x); noised_x_II[context_node_mask_II], noised_x_II[target_node_mask_II] = noised_x_context, noised_x_target
        
            noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
            noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
            noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr); noised_edge_attr_II[context_edge_mask], noised_edge_attr_II[target_edge_mask] = noised_edge_attr_context, noised_edge_attr_target
        
            noised_data_II = clean_batch.clone(); noised_data_II.pos, noised_data_II.x, noised_data_II.edge_attr = noised_pos_II, noised_x_II, noised_edge_attr_II
        
            predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

            lossII_a = calculate_atom_type_loss(predictions_II['atom_type_logits'], clean_batch.x[target_node_mask_II].argmax(dim=-1), t2_per_node[target_node_mask_II], args.lambda_aux, scheduler.T_full)
            lossII_r = calculate_coordinate_loss_wrapper(predictions_II['predicted_r0'], noise2[target_node_mask_II], noised_pos_target, t2_per_node[target_node_mask_II], scheduler, 'delta')
            lossII_b = calculate_bond_type_loss(predictions_II['bond_logits'], clean_batch.edge_attr[target_edge_mask].argmax(dim=-1), t2_per_edge[target_edge_mask], args.lambda_aux, scheduler.T_full)
            loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b

            # --- æ€»éªŒè¯æŸå¤± ---
            total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II 

            reward = (-total_loss - log_prob_orders).detach()  # å¥–åŠ±å¿…é¡»ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å¸¦æ¢¯åº¦

            # ç­–ç•¥æŸå¤± L_policy = -R(Ï€) * log q_Ï†(Ï€|G)
            loss_s_model = (-reward * log_prob_orders).mean()
        
        total_val_loss += total_loss.item()
        total_s_val_loss_epoch += loss_s_model.item()
        pbar_val.set_postfix({
            'loss_G': f"{total_loss.item():.2f}", 
            'loss_S': f"{loss_s_model.item():.2f}"
        })
    
    avg_val_loss = total_val_loss / len(val_loader)
    avg_s_val_loss = total_s_val_loss_epoch / len(val_loader)
    return avg_val_loss, avg_s_val_loss

def seg(batch_fc, orders, fragmenter):
    """
    ä»æ‰¹æ¬¡åˆ†å­å›¾å’Œå¯¹åº”çš„æ’åºåˆ—è¡¨ç”Ÿæˆå­å›¾æ‰¹æ¬¡
    
    Args:
        batch_fc: ä»DataLoaderè·å–çš„æ‰¹æ¬¡åˆ†å­å›¾ï¼ˆPyGçš„Batchå¯¹è±¡ï¼‰
        orders: ä¸æ‰¹æ¬¡å¯¹åº”çš„æ’åºåˆ—è¡¨é›†åˆï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåˆ†å­å›¾çš„èŠ‚ç‚¹æ’åº
        
    Returns:
        clean_batch: ç”±å­å›¾ç»„æˆçš„æ–°æ‰¹æ¬¡ï¼ˆPyGçš„Batchå¯¹è±¡ï¼‰
    """
    # ç¡®ä¿è¾“å…¥æ‰¹æ¬¡å¤§å°ä¸æ’åºæ•°é‡ä¸€è‡´
    assert len(orders) == batch_fc.num_graphs, \
        f"æ’åºæ•°é‡({len(orders)})ä¸æ‰¹æ¬¡ä¸­åˆ†å­å›¾æ•°é‡({batch_fc.num_graphs})ä¸åŒ¹é…"
    
    # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„å­å›¾
    subgraphs = []
    
    # æ‹†åˆ†æ‰¹æ¬¡ä¸ºå•ä¸ªåˆ†å­å›¾ï¼ˆä½¿ç”¨PyGçš„unbindæ–¹æ³•ï¼‰
    single_graphs = batch_fc.to_data_list()
    
    # å¾ªç¯å¤„ç†æ¯ä¸ªåˆ†å­å›¾åŠå…¶æ’åº
    for graph, order in zip(single_graphs, orders):
        # ç”Ÿæˆå­å›¾
        subgraph = fragmenter(graph, order)
        subgraphs.append(subgraph)
    
    # å°†æ‰€æœ‰å­å›¾ç»„åˆæˆæ–°çš„æ‰¹æ¬¡ï¼ˆä½¿ç”¨PyGçš„Batch.from_data_listæ–¹æ³•ï¼‰
    from torch_geometric.data import Batch
    clean_batch = Batch.from_data_list(subgraphs)
    
    return clean_batch.to(batch_fc.x.device)


# ==============================================================================
# 4. ä¸»è®­ç»ƒå‡½æ•° (Main Training Function)
# ==============================================================================

def train(
    args,
    logger,
    train_loader, # è®­ç»ƒæ•°æ®
    val_loader, # æµ‹è¯•æ•°æ®
    s_model, # æ’åºç½‘ç»œå®ä¾‹
    model, # E_DiT_Network å®ä¾‹
    scheduler, # HierarchicalDiffusionScheduler å®ä¾‹
    subgraph_generator_fn,
    amp_autocast,
    loss_scaler
):
    """
    ä¸»è®­ç»ƒå‡½æ•°ã€‚
    """
    device = args.device
    model.to(device)
    s_model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_model = optim.Adam(model.parameters(), lr=args.learning_rate)
    optimizer_s_model = optim.Adam(s_model.parameters(), lr=args.s_learning_rate) # s_modelæœ‰ç‹¬ç«‹çš„å­¦ä¹ ç‡

    # åˆ›å»º CosineAnnealingLR è°ƒåº¦å™¨
    T_max = args.epochs  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸ºæ€» epoch æ•°
    lr_min_factor = args.lr_min_factor
    scheduler_model = CosineAnnealingLR(optimizer_model, T_max=T_max, eta_min=lr_min_factor * args.learning_rate)
    scheduler_s_model = CosineAnnealingLR(optimizer_s_model, T_max=T_max, eta_min=lr_min_factor * args.s_learning_rate)

    best_val_loss = float('inf')
    best_epoch = 0

    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨: {args.checkpoints_dir}")
    logger.info("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(1, args.epochs + 1):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºâ€œè®­ç»ƒæ¨¡å¼â€
        # å®ƒä¼šé€šçŸ¥æ¨¡å‹ä¸­æ‰€æœ‰å…·æœ‰ä¸åŒè®­ç»ƒ/è¯„ä¼°è¡Œä¸ºçš„å±‚ï¼ˆä¸»è¦æ˜¯ Dropout å±‚å’Œ BatchNorm å±‚ï¼‰åˆ‡æ¢åˆ°å®ƒä»¬çš„è®­ç»ƒçŠ¶æ€ã€‚
        # Dropout å±‚åœ¨è®­ç»ƒæ—¶ä¼šéšæœºâ€œä¸¢å¼ƒâ€ä¸€äº›ç¥ç»å…ƒï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¸ä¼šä¸¢å¼ƒï¼Œä¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒã€‚
        # BatchNorm å±‚åœ¨è®­ç»ƒæ—¶ä¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ›´æ–°å…¶å†…éƒ¨çš„å…¨å±€ç»Ÿè®¡é‡ï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¼šä½¿ç”¨å·²å­¦ä¹ åˆ°çš„å…¨å±€ç»Ÿè®¡é‡ã€‚
        model.train()  
        s_model.train()

        total_loss_epoch = 0.0
        total_s_loss_epoch = 0.0
        
        # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
        pbar_train = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for step, (batch_non_fc, batch_fc) in enumerate(pbar_train):
            batch_fc = batch_fc.to(device)
            batch_non_fc = batch_non_fc.to(device) # å°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¼ é‡ï¼Œå¦‚ pos, x, edge_index ç­‰ï¼‰ä¸€æ¬¡æ€§åœ°ç§»åŠ¨åˆ°ä¹‹å‰å®šä¹‰å¥½çš„ç›®æ ‡è®¾å¤‡ device ä¸Š
             
            # åœ¨æ¯ä¸ª step å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            optimizer_model.zero_grad()
            optimizer_s_model.zero_grad()
            with torch.autograd.set_detect_anomaly(True):

                # --- ä½¿ç”¨ amp_autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ ---
                # å°†æ‰€æœ‰çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—åŒ…è£¹åœ¨å†…
                with amp_autocast():

                    # å°†éå…¨è¿æ¥çš„ batch é€å…¥æ’åºç½‘ç»œï¼Œè¿”å›æ’åºå’ŒæŸå¤±
                    orders, log_prob_orders = s_model(batch_non_fc)

                    # å¾—åˆ°åˆ‡åˆ†åçš„å­å›¾ç‰‡æ®µ
                    clean_batch = seg(batch_fc, orders, subgraph_generator_fn)

                    # --- 0. å‡†å¤‡å·¥ä½œ ---
                    num_graphs = clean_batch.num_graphs # æ‰¹æ¬¡ä¸­åŒ…å«çš„ç‹¬ç«‹å›¾çš„æ•°é‡ï¼ˆç­‰äº batch_sizeï¼‰ã€‚ç”¨äºé‡‡æ ·å›¾çº§åˆ«çš„å˜é‡ï¼Œå¦‚æ—¶é—´æ­¥ t
                    num_nodes = clean_batch.num_nodes #  æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„èŠ‚ç‚¹æ€»æ•°
                    num_edges = clean_batch.num_edges # æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„è¾¹æ€»æ•°

                    # a. åæ ‡ç¼©æ”¾
                    scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)

                    # b. é‡‡æ ·æ—¶é—´æ­¥å’Œé«˜æ–¯å™ªå£°
                    # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªå›¾éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ t1
                    # t1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [batch_size] çš„å¼ é‡ï¼Œä¾‹å¦‚ tensor([18, 98, 21, ...])
                    t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device)
                    t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
                    # noise1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, 3] çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ã€‚noise1[i] å°±æ˜¯è¦åŠ åˆ°ç¬¬ i ä¸ªåŸå­åæ ‡ä¸Šçš„å™ªå£°å‘é‡ã€‚
                    noise1 = torch.randn_like(scaled_pos)
                    noise2 = torch.randn_like(scaled_pos)

                    # c. å°† per-graph çš„æ—¶é—´æ­¥æ‰©å±•åˆ° per-node å’Œ per-edge
                    # t1: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_graphs] çš„å¼ é‡ã€‚å‡è®¾ batch_size=4ï¼Œt1å¯èƒ½é•¿è¿™æ ·ï¼štensor([18, 98, 21, 76])
                    # clean_batch.batch: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_nodes] çš„å¼ é‡ï¼Œè®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªä¸ªå›¾ã€‚
                    # å®ƒå¯èƒ½é•¿è¿™æ ·ï¼ˆå‡è®¾4ä¸ªå›¾åˆ†åˆ«æœ‰3, 2, 4, 3ä¸ªèŠ‚ç‚¹ï¼‰ï¼štensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3])ã€‚
                    t1_per_node = t1[clean_batch.batch] # å½¢çŠ¶ä¸º[num_nodes]
                    t1_per_edge = t1[clean_batch.batch[clean_batch.edge_index[0]]] # å½¢çŠ¶ä¸º[num_edges]


                    # --- ç­–ç•¥ I: å…¨å±€å»å™ª (ç”Ÿæˆå™ªå£°å›¾ â… ) ---

                    # a. åŠ å™ªåæ ‡
                    noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, schedule_type='alpha')

                    # b. åŠ å™ªåŸå­ç±»å‹
                    noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)

                    # c. åŠ å™ªè¾¹å±æ€§
                    noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)

                    # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â… 
                    # å¤åˆ¶å¹²å‡€çš„æ•°æ®ï¼Œæ›´æ”¹åŠ å™ªçš„éƒ¨åˆ†
                    noised_data_I = clean_batch.clone()
                    noised_data_I.pos = noised_pos_I
                    noised_data_I.x = noised_x_I
                    noised_data_I.edge_attr = noised_edge_attr_I

                    # e. å‡†å¤‡æ¨¡å‹è¾“å…¥
                    # åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºå½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰åŸå­çš„æ€»æ•°ï¼Œå†…å®¹å…¨ä¸º True çš„å‘é‡ã€‚
                    target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
                    # å¤„ç†å¹¶è¾“å‡ºæ‰€æœ‰è¾¹çš„é¢„æµ‹ç»“æœ
                    target_edge_mask_I = torch.ones(num_edges, dtype=torch.bool, device=device)

                    # f. æ¨¡å‹å‰å‘ä¼ æ’­
                    predictions_I = model(noised_data_I, t1, target_node_mask_I, target_edge_mask_I)

                    # g. è®¡ç®—æŸå¤± â… 
                    lossI_a = calculate_atom_type_loss(
                        predictions_I['atom_type_logits'],
                        clean_batch.x.argmax(dim=-1),  # ä» One-Hot ç¼–ç çš„ç‰¹å¾å¼ é‡ä¸­ï¼Œæå–å‡ºæ¯ä¸ªé¡¹ç›®å¯¹åº”çš„ç±»åˆ«ç´¢å¼• (class index)
                        args.lambda_aux
                    )
                    lossI_r = calculate_coordinate_loss_wrapper(
                        predicted_r0=predictions_I['predicted_r0'],
                        true_noise=noise1,
                        r_t=noised_pos_I,
                        t=t1_per_node,
                        scheduler=scheduler,
                        schedule_type='alpha'
                    )
                    lossI_b = calculate_bond_type_loss(
                        pred_logits=predictions_I['bond_logits'],
                        true_b0_indices=clean_batch.edge_attr.argmax(dim=-1),
                        lambda_aux=args.lambda_aux
                    )
                    loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b


                    # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ (ç”Ÿæˆå™ªå£°å›¾ â…¡) ---

                    # a. è¯†åˆ«ä¸Šä¸‹æ–‡å’Œç›®æ ‡
                    # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç›®æ ‡
                    target_node_mask_II = clean_batch.is_new_node.squeeze() # is_new_node å°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡maskï¼Œç»´åº¦å‹ç¼©ä¸º[num_nodes]
                    target_node_mask_II = target_node_mask_II.to(torch.bool)
                    # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯ä¸Šä¸‹æ–‡èŠ‚ç‚¹ï¼Œç”¨äºå¯¹ä¸Šä¸‹æ–‡èŠ‚ç‚¹åŠ å™ª
                    context_node_mask_II = ~target_node_mask_II
                    # æ ‡è¯†å“ªäº›è¾¹æ˜¯ä¸é¢„æµ‹ç›®æ ‡èŠ‚ç‚¹ç›¸å…³çš„è¾¹
                    # å¯¹äºç¬¬ i æ¡è¾¹ï¼Œå¦‚æœå®ƒçš„èµ·ç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹æˆ–è€…å®ƒçš„ç»ˆç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯éœ€è¦è¢«é¢„æµ‹çš„è¾¹
                    target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
                    # ç”¨äºå¯¹ä¸Šä¸‹æ–‡è¾¹åŠ å™ª
                    context_edge_mask = ~target_edge_mask

                    # b. å‡†å¤‡æ—¶é—´æ­¥ (T1 å’Œ t2)
                    # åˆ›å»ºä¸€ä¸ªä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒã€ç±»å‹ç›¸åŒã€è®¾å¤‡ç›¸åŒçš„æ–°å¼ é‡ï¼Œå¹¶å°†æ‰€æœ‰å…ƒç´ å¡«å……ä¸ºT1
                    t_T1_per_node = torch.full_like(t1_per_node, fill_value=scheduler.T1)
                    t_T1_per_edge = torch.full_like(t1_per_edge, fill_value=scheduler.T1)
                    t2_per_node = t2[clean_batch.batch]
                    t2_per_edge = t2[clean_batch.batch[clean_batch.edge_index[0]]]

                    # c. å¯¹ä¸Šä¸‹æ–‡å’Œç›®æ ‡åˆ†åˆ«åŠ å™ª
                    # åæ ‡
                    # è®¡ç®—å‡ºæ‰€æœ‰ä¸Šä¸‹æ–‡åŸå­çš„åŠ å™ªååæ ‡
                    noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
                    # è®¡ç®—å‡ºæ‰€æœ‰ç›®æ ‡åŸå­çš„åŠ å™ªååæ ‡
                    noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
                    # åˆ›å»ºä¸€ä¸ªç©ºçš„â€œç”»å¸ƒâ€
                    noised_pos_II = torch.zeros_like(scaled_pos)
                    # å°†è®¡ç®—å¥½çš„ä¸Šä¸‹æ–‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                    noised_pos_II[context_node_mask_II] = noised_pos_context
                    # å°†è®¡ç®—å¥½çš„ç›®æ ‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                    noised_pos_II[target_node_mask_II] = noised_pos_target

                    # åŸå­ç±»å‹
                    noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
                    noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
                    noised_x_II = torch.zeros_like(clean_batch.x)
                    noised_x_II[context_node_mask_II] = noised_x_context
                    noised_x_II[target_node_mask_II] = noised_x_target

                    # è¾¹å±æ€§
                    noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
                    noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
                    noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr)
                    noised_edge_attr_II[context_edge_mask] = noised_edge_attr_context
                    noised_edge_attr_II[target_edge_mask] = noised_edge_attr_target

                    # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â…¡
                    noised_data_II = clean_batch.clone()
                    noised_data_II.pos = noised_pos_II
                    noised_data_II.x = noised_x_II
                    noised_data_II.edge_attr = noised_edge_attr_II


                    # f. æ¨¡å‹å‰å‘ä¼ æ’­ (æ³¨æ„æ—¶é—´æ­¥ä¼ å…¥çš„æ˜¯ t2)
                    predictions_II = model(noised_data_II, t2, target_node_mask_II, target_edge_mask)

                    # g. è®¡ç®—æŸå¤± â…¡
                    # æ³¨æ„ï¼šè¿™é‡Œçš„çœŸå®æ ‡ç­¾å’Œå™ªå£°éƒ½éœ€è¦æ ¹æ® mask è¿›è¡Œç­›é€‰
                    lossII_a = calculate_atom_type_loss(
                        predictions_II['atom_type_logits'],
                        clean_batch.x[target_node_mask_II].argmax(dim=-1),
                        args.lambda_aux
                    )
                    lossII_r = calculate_coordinate_loss_wrapper(
                        predicted_r0=predictions_II['predicted_r0'],
                        true_noise=noise2[target_node_mask_II],
                        r_t=noised_pos_target,
                        t=t2_per_node[target_node_mask_II],
                        scheduler=scheduler,
                        schedule_type='delta'
                    )
                    lossII_b = calculate_bond_type_loss(
                        pred_logits=predictions_II['bond_logits'],
                        true_b0_indices=clean_batch.edge_attr[target_edge_mask].argmax(dim=-1),
                        lambda_aux=args.lambda_aux
                    )
                    loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b


                    # --- æ€»æŸå¤±ä¸åå‘ä¼ æ’­ ---
                    total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II

                     # æ ¹æ® OM è®ºæ–‡, å¥–åŠ± R(Ï€) = log p_Î¸(G|Ï€) - log q_Ï†(Ï€|G)
                    # log p_Î¸(G|Ï€) ç”±ç”Ÿæˆæ¨¡å‹çš„è´ŸæŸå¤± -total_loss è¿‘ä¼¼
                    # log q_Ï†(Ï€|G) æ˜¯æ’åºç½‘ç»œè¾“å‡ºçš„ log_prob_orders
                    # æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ– E[R(Ï€)], æ‰€ä»¥å°† R(Ï€) ä½œä¸º REINFORCE çš„å¥–åŠ±
                    reward = (-total_loss - log_prob_orders).detach()  # å¥–åŠ±å¿…é¡»ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å¸¦æ¢¯åº¦

                    # ç­–ç•¥æŸå¤± L_policy = -R(Ï€) * log q_Ï†(Ï€|G)
                    loss_s_model = (-reward * log_prob_orders).mean()


                # --- 4. åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–° ---
                # ä½¿ç”¨ loss_scaler åˆ†åˆ«ç¼©æ”¾ä¸¤ä¸ªæ¨¡å‹çš„æŸå¤±
                # backward() ä¼šç´¯åŠ æ¢¯åº¦ï¼Œè€Œæˆ‘ä»¬å·²ç»åœ¨ step å¼€å§‹æ—¶æ¸…é›¶äº†æ¢¯åº¦

                if loss_scaler is not None:
                    # --- AMP å¯ç”¨æ¨¡å¼ ---
                    # 1. åˆ†åˆ«å¯¹ä¸¤ä¸ªæŸå¤±è¿›è¡Œç¼©æ”¾å’Œåå‘ä¼ æ’­
                    loss_scaler._scaler.scale(total_loss).backward()
                    loss_scaler._scaler.scale(loss_s_model).backward()

                    # 2. å¯¹ model çš„æ¢¯åº¦è¿›è¡Œåç¼©æ”¾ï¼Œç„¶åè£å‰ª
                    loss_scaler._scaler.unscale_(optimizer_model)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                    # 3. å¯¹ s_model çš„æ¢¯åº¦è¿›è¡Œåç¼©æ”¾ï¼Œç„¶åè£å‰ª
                    loss_scaler._scaler.unscale_(optimizer_s_model)
                    torch.nn.utils.clip_grad_norm_(s_model.parameters(), max_norm=1.0)

                    # 4. æ›´æ–°ä¸¤ä¸ªä¼˜åŒ–å™¨
                    loss_scaler._scaler.step(optimizer_model)
                    loss_scaler._scaler.step(optimizer_s_model)

                    # 5. æ›´æ–°ç¼©æ”¾å™¨
                    loss_scaler._scaler.update()
                else:
                    # --- AMP ç¦ç”¨æ¨¡å¼ (æ ‡å‡† float32 è®­ç»ƒ) ---
                    # 1. ç›´æ¥å¯¹ä¸¤ä¸ªæŸå¤±è¿›è¡Œåå‘ä¼ æ’­
                    total_loss.backward()
                    loss_s_model.backward()

                    # 2. ç›´æ¥å¯¹ä¸¤ä¸ªæ¨¡å‹çš„æ¢¯åº¦è¿›è¡Œè£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    torch.nn.utils.clip_grad_norm_(s_model.parameters(), max_norm=1.0)

                    # 3. æ›´æ–°ä¸¤ä¸ªä¼˜åŒ–å™¨
                    optimizer_model.step()
                    optimizer_s_model.step()

            
            total_loss_epoch += total_loss.item()
            total_s_loss_epoch += loss_s_model.item()
            pbar_train.set_postfix({
                'loss_G': f"{total_loss.item():.2f}", 
                'loss_S': f"{loss_s_model.item():.2f}"
            })
            
        avg_train_loss = total_loss_epoch / len(train_loader)
        avg_s_train_loss = total_s_loss_epoch / len(train_loader)
        logger.info(f"Epoch {epoch} [Train] å®Œæˆ, å¹³å‡ç”ŸæˆæŸå¤±: {avg_train_loss:.4f}, å¹³å‡æ’åºæŸå¤±: {avg_s_train_loss:.4f}")


        # --- éªŒè¯é˜¶æ®µ ---
        if epoch >= args.val_thre and (epoch % args.val_log_freq == 0):
            avg_val_loss, avg_s_val_loss = validate(val_loader, model, s_model, scheduler, args, amp_autocast, subgraph_generator_fn)
            logger.info(f"Epoch {epoch} [Validation] å®Œæˆ, å¹³å‡æŸå¤±: {avg_val_loss:.4f}")

            # ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹ 
            logger.info(f"åœ¨ Epoch {epoch} ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹åŠå…¶éªŒè¯æŸå¤±...")

            checkpoint_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                's_model_state_dict': s_model.state_dict(),
                'optimizer_model_state_dict': optimizer_model.state_dict(),
                'optimizer_s_model_state_dict': optimizer_s_model.state_dict(),
                'scheduler_model_state_dict': scheduler_model.state_dict(),    
                'scheduler_s_model_state_dict': scheduler_s_model.state_dict(), 
                'loss_scaler_state_dict': loss_scaler.state_dict(),
                'validation_loss': avg_val_loss, 
                's_validation_loss': avg_s_val_loss,
                'args': args
            }
            # ä½¿ç”¨åŒ…å« epoch ç¼–å·çš„å”¯ä¸€æ–‡ä»¶å
            checkpoint_path = os.path.join(args.checkpoints_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint_state, checkpoint_path)

            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_epoch = epoch
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜æœ€ä½³æ¨¡å‹...")
                
                # ä¸ºæœ€ä½³æ¨¡å‹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ä¿å­˜çŠ¶æ€
                best_model_state = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    's_model_state_dict': s_model.state_dict(),
                    'best_val_loss': best_val_loss,
                    'args': args
                }
                
                best_model_path = os.path.join(args.checkpoints_dir, 'best_model.pth')
                torch.save(best_model_state, best_model_path)

            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_model.step()
            scheduler_s_model.step()

    logger.info("è®­ç»ƒå®Œæˆã€‚")
    logger.info(f"æœ€ç»ˆï¼Œæœ€ä½³æ¨¡å‹å‘ç°åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º: {best_val_loss:.4f}")
