import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Batch
from torch_geometric.utils import softmax, to_dense_batch
from tqdm import tqdm
import torch.nn.functional as F
from typing import Tuple
import os
# from torch.optim.lr_scheduler import ReduceLROnPlateau # <-- æ–°å¢: å¯¼å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import CosineAnnealingLR  # å¯¼å…¥ CosineAnnealingLR


# å‡è®¾æ¨¡å‹å’Œè°ƒåº¦å™¨å·²ç»å®šä¹‰å¥½
# from e_dit_network import E_DiT_Network
from scheduler import HierarchicalDiffusionScheduler

# ==============================================================================
# 1. è¾…åŠ©å‡½æ•° (Helper Functions)
# ==============================================================================

def scale_to_unit_sphere(pos: torch.Tensor, batch_map: torch.Tensor) -> torch.Tensor:
    """
    å°†æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾çš„åæ ‡ç‹¬ç«‹åœ°ç¼©æ”¾åˆ°å•ä½çƒå†…ã€‚
    
    Args:
        pos (torch.Tensor): æ‰¹æ¬¡ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åæ ‡å¼ é‡, shape [N, 3]ã€‚
        batch_map (torch.Tensor): å°†æ¯ä¸ªèŠ‚ç‚¹æ˜ å°„åˆ°å…¶æ‰€å±å›¾çš„å‘é‡, shape [N]ã€‚
    
    Returns:
        torch.Tensor: ç¼©æ”¾åçš„åæ ‡ã€‚
    """
    # å‡è®¾æˆ‘ä»¬çš„è¾“å…¥ pos æ˜¯ä¸€ä¸ª [N, 3] çš„å¼ é‡ï¼Œä»£è¡¨æ‰¹æ¬¡ä¸­æ‰€æœ‰ N ä¸ªåŸå­çš„åæ ‡ï¼›
    # batch_map æ˜¯ä¸€ä¸ª [N] çš„å¼ é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªåŸå­å±äºå“ªä¸ªåˆ†å­å›¾ï¼ˆ0, 0, 0, 1, 1, ...ï¼‰ã€‚
    # PyG çš„ scatter å‡½æ•°å¯ä»¥é«˜æ•ˆåœ°æŒ‰ç»„æ±‚å’Œ/æ±‚å‡å€¼
    from torch_geometric.utils import scatter
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åˆ°å…¶è´¨å¿ƒçš„è·ç¦»
    # è¾“å…¥ pos çš„ç»´åº¦æ˜¯ [N, 3]
    # e.g. å¯¹äºç¬¬ 0 è¡Œ [x_0, y_0, z_0]ï¼Œå®ƒè®¡ç®— sqrt(x_0Â² + y_0Â² + z_0Â²)
    # torch.linalg.norm æ²¿ç€ dim=1 è¿›è¡Œæ“ä½œï¼Œè¿™ä¸ªç»´åº¦åœ¨è®¡ç®—åä¼šæ¶ˆå¤±
    # è¾“å‡º distances çš„ç»´åº¦æ˜¯ [N]
    distances = torch.linalg.norm(pos, dim=1)
    
    # æŒ‰å›¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå›¾ä¸­çš„æœ€å¤§è·ç¦»
    max_distances = scatter(distances, batch_map, dim=0, reduce='max') # shape: [num_graphs]
    
    # è®¡ç®—æ¯ä¸ªå›¾çš„ç¼©æ”¾å› å­ï¼ŒåŠ ä¸Šä¸€ä¸ªå°çš„ epsilon é˜²æ­¢é™¤ä»¥é›¶
    scale_factors = max_distances[batch_map].unsqueeze(1) + 1e-8

    
    # ç¼©æ”¾åæ ‡
    return pos / scale_factors


def noise_discrete_features(
    features_0: torch.Tensor,
    Q_bar: torch.Tensor,
    t_per_item: torch.Tensor
) -> torch.Tensor:
    """
    å¯¹ one-hot ç¼–ç çš„ç¦»æ•£ç‰¹å¾ï¼ˆå¦‚åŸå­ç±»å‹ã€è¾¹ç±»å‹ï¼‰è¿›è¡ŒåŠ å™ªã€‚

    Args:
        features_0 (torch.Tensor): å¹²å‡€çš„ one-hot ç‰¹å¾, shape [M, K] (Mä¸ªé¡¹ç›®, Kä¸ªç±»åˆ«)ã€‚
        Q_bar (torch.Tensor):      è½¬ç§»çŸ©é˜µé›†åˆ, shape [T, K, K] (Tä¸ªæ—¶é—´æ­¥)ã€‚
        t_per_item (torch.Tensor): æ¯ä¸ªé¡¹ç›®å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]ã€‚

    Returns:
        torch.Tensor: åŠ å™ªåçš„ one-hot ç‰¹å¾ã€‚
    """
    # 1. æ ¹æ®æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ tï¼Œä» Q_bar ä¸­é€‰å‡ºå¯¹åº”çš„è½¬ç§»çŸ©é˜µ
    # Q_bar_t çš„ shape ä¸º [M, K, K]
    Q_bar_t = Q_bar[t_per_item] 
    
    # 2. è®¡ç®—åŠ å™ªåçš„æ¦‚ç‡åˆ†å¸ƒ
    # features_0.unsqueeze(1) -> [M, 1, K]   åœ¨ features_0 çš„ç¬¬1ä¸ªç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
    # Q_bar_t                 -> [M, K, K]
    # prob_t                  -> [M, 1, K]
    # æ‰§è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
    prob_t = torch.bmm(features_0.unsqueeze(1), Q_bar_t).squeeze(1) # shape: [M, K]
    
    # 3. æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°æ–°çš„ç±»åˆ«ç´¢å¼•
    # torch.multinomial è¦æ±‚è¾“å…¥æ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬è¿™é‡Œå·²ç»æ˜¯æ¦‚ç‡äº†
    # multinomial ä¼šæŠŠæ¯ä¸€è¡Œéƒ½çœ‹ä½œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„â€œéª°å­â€çš„æ¦‚ç‡è®¾ç½®
    # num_samples=1 æŒ‡æ¯è¡Œé‡‡æ ·ä¸€æ¬¡ï¼Œè¿™æ—¶è¾“å‡ºç»´åº¦ä¸º[M, 1]
    # .squeeze(-1) ä½œç”¨ä¸ºç§»é™¤æœ€åä¸€ä¸ªç»´åº¦ï¼ˆdim=-1ï¼‰ä¸Šå¤§å°ä¸º1çš„ç»´åº¦
    sampled_indices = torch.multinomial(prob_t, num_samples=1).squeeze(-1) # shape: [M]
    
    # 4. å°†é‡‡æ ·å‡ºçš„ç´¢å¼•è½¬æ¢å› one-hot ç¼–ç 
    num_classes = features_0.shape[1]  # è·å–ç±»åˆ«çš„æ€»æ•°
    # å°†æ•´æ•°ç±»åˆ«ç´¢å¼•è½¬æ¢æˆ One-Hot ç¼–ç å‘é‡
    features_t = torch.nn.functional.one_hot(sampled_indices, num_classes=num_classes).float()
    
    return features_t

# ==============================================================================
# 2. æŸå¤±å‡½æ•°æ¡†æ¶ (Loss Function Skeletons)
# ==============================================================================

# 2.1 åŸå­ç±»å‹æŸå¤±
# æ¨èä½¿ç”¨çš„ã€æ›´ç®€æ´çš„æŸå¤±å‡½æ•°
def calculate_atom_type_loss(
    pred_logits: torch.Tensor,   # æ¨¡å‹å¯¹ x0 çš„é¢„æµ‹ logits, shape [M, C]
    true_x0_indices: torch.Tensor, # çœŸå®çš„ x0 ç±»åˆ«ç´¢å¼•, shape [M]
    t: torch.Tensor,               # æ—¶é—´æ­¥, shape [M]
    lambda_aux: float = 0.001,     # D3PMè®ºæ–‡å»ºè®®çš„å°å€¼
    T: int = 1000                  # æ€»æ—¶é—´æ­¥é•¿ï¼Œç”¨äºæƒé‡
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆåŸå­ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_x0_indices: çœŸå®çš„ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªé¡¹ç›®çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """
    # 1. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤± (å¯¹åº”äº L_vlb çš„ä¸»è¦éƒ¨åˆ†å’Œ L_aux)
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_x0_indices, reduction='none')
    
    # 2. æ ¹æ® D3PM æ··åˆæŸå¤± L_Î» çš„æ€æƒ³ï¼Œåº”ç”¨æƒé‡
    # L_Î» = L_vlb + Î» * [-log p(x0|xt)]
    # L_vlb çš„ KL é¡¹åœ¨ t>1 æ—¶æƒé‡ä¸º1ï¼Œåœ¨ t=1 æ—¶æƒé‡ä¸º1(é‡å»ºé¡¹)ã€‚
    # L_aux åœ¨æ‰€æœ‰ t ä¸Šæƒé‡éƒ½ä¸º Î»ã€‚
    # æ‰€ä»¥ï¼Œæ€»æƒé‡ä¸º 1 + Î»ï¼Œé™¤äº† t=0 çš„æƒ…å†µï¼ˆæˆ‘ä»¬ä¸å¤„ç†ï¼‰ã€‚
    
    # ä¸€ä¸ªéå¸¸å¸¸è§çš„ç®€åŒ–å®ç°æ˜¯ç›´æ¥åº”ç”¨æƒé‡
    # å¦ä¸€ç§æ¥è‡ªå…¶ä»–è®ºæ–‡çš„æ€è·¯æ˜¯ç»™ä½tçš„æŸå¤±æ›´é«˜çš„æƒé‡
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ L_Î» çš„ç²¾ç¥ï¼šä¸€ä¸ªåŸºç¡€æŸå¤± + ä¸€ä¸ªå°çš„è¾…åŠ©æŸå¤±
    
    # æƒé‡ä¸º 1(æ¥è‡ªL_vlb) + lambda_aux (æ¥è‡ªL_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # å¯¹äº t=1 çš„ç‰¹æ®Šæƒ…å†µï¼ŒVLBä¸­åªæœ‰é‡å»ºé¡¹ï¼Œå¯ä»¥è®¤ä¸ºæƒé‡ä¸åŒ
    # ä½†D3PMçš„æ··åˆæŸå¤±ç®€åŒ–äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¹Ÿé‡‡ç”¨ç®€åŒ–
    
    return final_loss.mean()

# 2.2 åŸå­åæ ‡æŸå¤±
def calculate_coordinate_loss_wrapper(
    predicted_r0: torch.Tensor,      # æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ (t=0), shape [M, 3]
    true_noise: torch.Tensor,        # ç”¨äºç”Ÿæˆ r_t çš„çœŸå®é«˜æ–¯å™ªå£°, shape [M, 3]
    r_t: torch.Tensor,               # è¾“å…¥åˆ°æ¨¡å‹çš„åŠ å™ªåæ ‡ (t>0), shape [M, 3]
    t: torch.Tensor,                 # æ¯ä¸ªåæ ‡å¯¹åº”çš„æ—¶é—´æ­¥, shape [M]
    scheduler,                       # HierarchicalDiffusionScheduler å®ä¾‹
    schedule_type: str               # ä½¿ç”¨çš„è°ƒåº¦ç±»å‹, 'alpha' æˆ– 'delta'
) -> torch.Tensor:
    """
    è®¡ç®—åŸå­åæ ‡çš„æŸå¤±ã€‚

    è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œå®ƒæ¥æ”¶æ¨¡å‹é¢„æµ‹çš„ r0ï¼Œä½¿ç”¨è°ƒåº¦å™¨å°†å…¶è½¬æ¢ä¸º
    é¢„æµ‹çš„å™ªå£° epsilonï¼Œç„¶åè®¡ç®—ä¸çœŸå®å™ªå£°çš„ L2 æŸå¤±ã€‚

    Args:
        predicted_r0: æ¨¡å‹é¢„æµ‹çš„å¹²å‡€åæ ‡ã€‚
        true_noise: çœŸå®çš„å™ªå£°ã€‚
        r_t: åŠ å™ªåçš„åæ ‡ã€‚
        t: æ—¶é—´æ­¥ã€‚
        scheduler: å™ªå£°è°ƒåº¦å™¨å®ä¾‹ã€‚
        schedule_type: ä½¿ç”¨çš„è°ƒåº¦ç±»å‹ ('alpha' æˆ– 'delta')ã€‚

    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æ ‡é‡æŸå¤±å€¼ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚
    # åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­è¢«æŸç§æ–¹å¼ç§»é™¤äº†ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œè¿™å¯ä»¥é˜²æ­¢å‡ºé”™ã€‚
    if predicted_r0.shape[0] == 0:
        return torch.tensor(0.0, device=predicted_r0.device)

    # 2. è°ƒç”¨è°ƒåº¦å™¨çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä» predicted_r0 åæ¨å‡º predicted_noise
    predicted_noise = scheduler.get_predicted_noise_from_r0(
        r_t=r_t,
        t=t,
        predicted_r0=predicted_r0,
        schedule_type=schedule_type
    )

    # 3. è®¡ç®—é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°ä¹‹é—´çš„ L2 æŸå¤± (å‡æ–¹è¯¯å·®, Mean Squared Error)
    # F.mse_loss(A, B) ä¼šè®¡ç®— (A - B)^2 çš„æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ã€‚
    loss = F.mse_loss(predicted_noise, true_noise)
    
    return loss

# 2.3 è¾¹ç±»å‹æŸå¤±
def calculate_bond_type_loss(
    pred_logits: torch.Tensor,      # æ¨¡å‹å¯¹å¹²å‡€è¾¹ç±»å‹çš„é¢„æµ‹ logits, shape [M_edges, C_bonds]
    true_b0_indices: torch.Tensor, # çœŸå®çš„å¹²å‡€è¾¹ç±»å‹ç´¢å¼•, shape [M_edges]
    t: torch.Tensor,               # æ¯ä¸ªè¾¹å¯¹åº”çš„æ—¶é—´æ­¥, shape [M_edges]
    lambda_aux: float = 0.001,     # è¾…åŠ©æŸå¤±çš„æƒé‡
    T: int = 1000                  # æ€»æ—¶é—´æ­¥é•¿ (å¯é€‰ï¼Œç”¨äºæ›´å¤æ‚çš„åŠ æƒ)
) -> torch.Tensor:
    """
    è®¡ç®—åŸºäº D3PM æ··åˆæŸå¤± L_Î» çš„ç®€åŒ–ç‰ˆè¾¹ç±»å‹æŸå¤±ã€‚
    è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒçš„äº¤å‰ç†µæŸå¤±ï¼Œä¸åŸå­ç±»å‹æŸå¤±çš„é€»è¾‘å®Œå…¨ç›¸åŒã€‚

    Args:
        pred_logits: æ¨¡å‹çš„ logits è¾“å‡ºã€‚
        true_b0_indices: çœŸå®çš„è¾¹ç±»åˆ«ç´¢å¼•ã€‚
        t: æ¯ä¸ªè¾¹å¯¹åº”çš„æ—¶é—´æ­¥ã€‚
        lambda_aux: è¾…åŠ©æŸå¤±çš„æƒé‡ã€‚
        T: å™ªå£°è¿‡ç¨‹çš„æ€»æ­¥é•¿ (å½“å‰æœªä½¿ç”¨ï¼Œä¸ºæœªæ¥æ‰©å±•ä¿ç•™)ã€‚

    Returns:
        torch.Tensor: è¯¥æ‰¹æ¬¡çš„å¹³å‡æŸå¤±ã€‚
    """
    # 1. æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºã€‚å¦‚æœä¸€ä¸ªæ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦é¢„æµ‹çš„è¾¹ï¼Œåˆ™æŸå¤±ä¸º0ã€‚
    # è¿™åœ¨ç­–ç•¥IIä¸­ï¼Œå¦‚æœç›®æ ‡åŸå­æ˜¯å­¤ç«‹ç‚¹æ—¶å¯èƒ½å‘ç”Ÿã€‚
    if pred_logits.shape[0] == 0:
        return torch.tensor(0.0, device=pred_logits.device)
        
    # 2. è®¡ç®—æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±
    # reduction='none' è¡¨ç¤ºæˆ‘ä»¬ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯æ¡è¾¹è®¡ç®—ä¸€ä¸ªæŸå¤±å€¼
    loss = F.cross_entropy(pred_logits, true_b0_indices, reduction='none')
    
    # 3. åº”ç”¨ D3PM æ··åˆæŸå¤±çš„ç®€åŒ–æƒé‡
    # æ€»æƒé‡ = 1 (æ¥è‡ª L_vlb) + lambda_aux (æ¥è‡ª L_aux)
    final_loss = (1 + lambda_aux) * loss
    
    # 4. å¯¹æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è¾¹çš„æŸå¤±æ±‚å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ ‡é‡æŸå¤±
    return final_loss.mean()


# ==============================================================================
# 3. éªŒè¯å‡½æ•° (Validation Function)
# ==============================================================================
@torch.no_grad() # è£…é¥°å™¨ï¼Œè¡¨ç¤ºè¯¥å‡½æ•°å†…æ‰€æœ‰ torch è®¡ç®—éƒ½ä¸éœ€è¦è®°å½•æ¢¯åº¦
def validate(val_loader, model, s_model, scheduler, args, amp_autocast):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æŸå¤±ã€‚
    æ­¤å‡½æ•°çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—é€»è¾‘ä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€è‡´ã€‚
    """
    device = args.device
    model.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_val_loss = 0.0
    pbar_val = tqdm(val_loader, desc=f"Validating", leave=False)

    for batch_non_fc, batch_fc in pbar_val:
        batch_fc = batch_fc.to(device)
        batch_non_fc = batch_non_fc.to(device)

        with amp_autocast():
            # å°†éå…¨è¿æ¥çš„ batch é€å…¥æ’åºç½‘ç»œ
            orders, log_prob_orders = s_model(batch_non_fc)

            clean_batch = seg(orders, batch_fc)

            # --- [é€»è¾‘ä¸è®­ç»ƒå¾ªç¯å®Œå…¨ç›¸åŒ] ---

            # --- 0. å‡†å¤‡å·¥ä½œ ---
            num_graphs, num_nodes, num_edges = clean_batch.num_graphs, clean_batch.num_nodes, clean_batch.num_edges
            scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
            t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device)
            t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
            noise1, noise2 = torch.randn_like(scaled_pos), torch.randn_like(scaled_pos)
            t1_per_node, t1_per_edge = t1[clean_batch.batch], t1[clean_batch.batch[clean_batch.edge_index[0]]]

            # --- ç­–ç•¥ I: å…¨å±€å»å™ª ---
            noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, 'alpha')
            noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            noised_data_I = clean_batch.clone(); noised_data_I.pos, noised_data_I.x, noised_data_I.edge_attr = noised_pos_I, noised_x_I, noised_edge_attr_I
            
            target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
            edge_index_to_predict_I = clean_batch.edge_index
            
            predictions_I = model(noised_data_I, t1, target_node_mask_I, edge_index_to_predict_I)
            
            lossI_a = calculate_atom_type_loss(predictions_I['atom_type_logits'], clean_batch.x.argmax(dim=-1), t1_per_node, args.lambda_aux, scheduler.T_full)
            lossI_r = calculate_coordinate_loss_wrapper(predictions_I['predicted_r0'], noise1, noised_pos_I, t1_per_node, scheduler, 'alpha')
            lossI_b = calculate_bond_type_loss(predictions_I['bond_logits'], clean_batch.edge_attr.argmax(dim=-1), t1_per_edge, args.lambda_aux, scheduler.T_full)
            loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b
    
            # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ ---
            target_node_mask_II = clean_batch.is_last.squeeze().bool()
            context_node_mask_II = ~target_node_mask_II
            target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
            context_edge_mask = ~target_edge_mask
            
            t_T1_per_node, t_T1_per_edge = torch.full_like(t1_per_node, scheduler.T1), torch.full_like(t1_per_edge, scheduler.T1)
            t2_per_node, t2_per_edge = t2[clean_batch.batch], t2[clean_batch.batch[clean_batch.edge_index[0]]]
    
            noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
            noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
            noised_pos_II = torch.zeros_like(scaled_pos); noised_pos_II[context_node_mask_II], noised_pos_II[target_node_mask_II] = noised_pos_context, noised_pos_target
            
            noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
            noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
            noised_x_II = torch.zeros_like(clean_batch.x); noised_x_II[context_node_mask_II], noised_x_II[target_node_mask_II] = noised_x_context, noised_x_target
        
            noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
            noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
            noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr); noised_edge_attr_II[context_edge_mask], noised_edge_attr_II[target_edge_mask] = noised_edge_attr_context, noised_edge_attr_target
        
            noised_data_II = clean_batch.clone(); noised_data_II.pos, noised_data_II.x, noised_data_II.edge_attr = noised_pos_II, noised_x_II, noised_edge_attr_II
        
            edge_index_to_predict_II = clean_batch.edge_index[:, target_edge_mask]
        
            predictions_II = model(noised_data_II, t2, target_node_mask_II, edge_index_to_predict_II)

            lossII_a = calculate_atom_type_loss(predictions_II['atom_type_logits'], clean_batch.x[target_node_mask_II].argmax(dim=-1), t2_per_node[target_node_mask_II], args.lambda_aux, scheduler.T_full)
            lossII_r = calculate_coordinate_loss_wrapper(predictions_II['predicted_r0'], noise2[target_node_mask_II], noised_pos_target, t2_per_node[target_node_mask_II], scheduler, 'delta')
            lossII_b = calculate_bond_type_loss(predictions_II['bond_logits'], clean_batch.edge_attr[target_edge_mask].argmax(dim=-1), t2_per_edge[target_edge_mask], args.lambda_aux, scheduler.T_full)
            loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b

            # --- æ€»éªŒè¯æŸå¤± ---
            total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II 

            reward = (-total_loss - log_prob_orders).detach()  # å¥–åŠ±å¿…é¡»ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å¸¦æ¢¯åº¦

            # ç­–ç•¥æŸå¤± L_policy = -R(Ï€) * log q_Ï†(Ï€|G)
            loss_s_model = -reward * log_prob_orders
        
        total_val_loss += total_loss.item()
        total_s_val_loss_epoch += loss_s_model.item()
        pbar_val.set_postfix({
            'loss_G': f"{total_loss.item():.2f}", 
            'loss_S': f"{loss_s_model.item():.2f}"
        })
    
    avg_val_loss = total_val_loss / len(val_loader)
    avg_s_val_loss = total_s_val_loss_epoch / len(val_loader)
    return avg_val_loss, avg_s_val_loss

# def sample_orders_from_batch(  # å¾…ä¿®æ”¹
#     s_model: nn.Module, 
#     graph_batch: Batch,
#     amp_autocast
# ) -> Tuple[torch.Tensor, torch.Tensor]:
#     """
#     ä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•°ï¼Œä½¿ç”¨ä¼ å…¥çš„æ’åºç½‘ç»œ(s_model)ä¸ºä¸€æ‰¹PyGå›¾è¿›è¡Œè‡ªå›å½’é‡‡æ ·ã€‚

#     Args:
#         s_model (nn.Module): ä¸€ä¸ªå·²ç»å®ä¾‹åŒ–çš„GNNæ¨¡å‹ï¼Œå®ƒå°†ä½œä¸ºæ’åºç½‘ç»œçš„æ ¸å¿ƒã€‚
#                              å®ƒçš„ forward æ–¹æ³•éœ€è¦æ¥æ”¶ (augmented_x, edge_index, batch)ã€‚
#         graph_batch (Batch): ä¸€ä¸ªåŒ…å«Nä¸ªå›¾çš„PyG Batchå¯¹è±¡ã€‚

#     Returns:
#         orders (torch.Tensor): ä¸€ä¸ªå½¢çŠ¶ä¸º [N, max_nodes] çš„å¼ é‡ï¼Œ
#                                å­˜å‚¨äº†æ¯ä¸ªå›¾çš„èŠ‚ç‚¹æ’åºï¼ˆç›¸å¯¹ç´¢å¼•ï¼‰ã€‚
#                                æœªä½¿ç”¨çš„ä½ç½®ç”¨-1å¡«å……ã€‚
#         total_log_prob (torch.Tensor): ä¸€ä¸ªæ ‡é‡ï¼Œä»£è¡¨æ•´ä¸ªæ‰¹æ¬¡æ‰€æœ‰æ’åºçš„
#                                        æ€»å¯¹æ•°æ¦‚ç‡ä¹‹å’Œï¼Œç”¨äºè®¡ç®—æŸå¤±ã€‚
#     """
#     num_graphs = graph_batch.num_graphs
#     num_nodes = graph_batch.num_nodes
#     device = graph_batch.x.device

#     # 1. åˆå§‹åŒ–çŠ¶æ€
#     graph_sizes = torch.bincount(graph_batch.batch).to(device)
#     max_nodes = graph_sizes.max().item()

#     orders = torch.full((num_graphs, max_nodes), -1, dtype=torch.long, device=device)
#     log_probs_per_graph = torch.zeros(num_graphs, device=device)
#     mask_selected_nodes = torch.zeros(num_nodes, dtype=torch.bool, device=device)
#     mask_finished_graphs = torch.zeros(num_graphs, dtype=torch.bool, device=device)
    
#     # åˆ›å»ºä¸€ä¸ªå¯æ›´æ–°çš„ç‰¹å¾ï¼Œç”¨äºå‘Šè¯‰GNNå“ªäº›èŠ‚ç‚¹å·²è¢«é€‰æ‹©
#     # æ³¨æ„ï¼šs_modelçš„è¾“å…¥ç»´åº¦éœ€è¦æ˜¯ åŸå§‹ç‰¹å¾ç»´åº¦ + 1
#     selection_feature = torch.zeros(num_nodes, 1, device=device)

#     # 2. è‡ªå›å½’å¾ªç¯
#     for t in range(max_nodes):
#         if mask_finished_graphs.all():
#             break

#         # a. å‡†å¤‡GNNè¾“å…¥
#         augmented_x = torch.cat([graph_batch.x, selection_feature], dim=-1)

#         # b. è°ƒç”¨ä¼ å…¥çš„ s_model è¿›è¡Œå‰å‘ä¼ æ’­
#         with amp_autocast():
#             logits = s_model(augmented_x, graph_batch.edge_index, graph_batch.batch)

#         # c. åº”ç”¨æ©ç 
#         logits = logits.masked_fill(mask_selected_nodes, -torch.inf)
#         finished_nodes_mask = mask_finished_graphs[graph_batch.batch]
#         logits = logits.masked_fill(finished_nodes_mask, -torch.inf)
        
#         # d. é‡‡æ ·
#         probs = softmax(logits, graph_batch.batch)
#         dense_probs, _ = to_dense_batch(probs, graph_batch.batch)
#         dense_probs[mask_finished_graphs] = 0
        
#         # å¦‚æœä¸€ä¸ªå›¾çš„æ‰€æœ‰æœ‰æ•ˆæ¦‚ç‡éƒ½ä¸º0ï¼ˆå¯èƒ½å› ä¸ºlogitså…¨æ˜¯-infï¼‰ï¼Œmultinomialä¼šæŠ¥é”™
#         # æˆ‘ä»¬ç»™å®ƒä¸€ä¸ªå¾®å°çš„å‡åŒ€åˆ†å¸ƒæ¥é¿å…è¿™ç§æƒ…å†µ
#         is_all_zero = (dense_probs.sum(dim=-1) == 0) & (~mask_finished_graphs)
#         if is_all_zero.any():
#             uniform_dist = torch.ones_like(dense_probs[is_all_zero])
#             dense_probs[is_all_zero] = uniform_dist
            
#         next_node_rel_indices = torch.multinomial(dense_probs, num_samples=1).squeeze(-1)

#         # e. è®°å½•å¯¹æ•°æ¦‚ç‡
#         sampled_probs = dense_probs.gather(1, next_node_rel_indices.unsqueeze(1)).squeeze(1)
#         step_log_probs = torch.log(sampled_probs + 1e-10)
#         log_probs_per_graph += step_log_probs * (~mask_finished_graphs)
        
#         # f. æ›´æ–°çŠ¶æ€
#         # (è¿™éƒ¨åˆ†ä»£ç ä¸ä¹‹å‰ç›¸åŒï¼Œç”¨äºå°†ç›¸å¯¹ç´¢å¼•è½¬ä¸ºç»å¯¹ç´¢å¼•å¹¶æ›´æ–°æ©ç )
#         _, node_mask = to_dense_batch(torch.arange(num_nodes, device=device), graph_batch.batch, fill_value=-1)
#         node_indices_matrix = torch.full(node_mask.shape, -1, dtype=torch.long, device=device)
#         node_indices_matrix[node_mask] = torch.arange(num_nodes, device=device)
#         abs_indices = node_indices_matrix.gather(1, next_node_rel_indices.unsqueeze(1)).squeeze(1)

#         valid_abs_indices = abs_indices[~mask_finished_graphs]
#         if valid_abs_indices.numel() > 0:
#             mask_selected_nodes[valid_abs_indices] = True
#             selection_feature[valid_abs_indices] = t + 1
            
#         orders[:, t] = next_node_rel_indices
#         mask_finished_graphs = (t + 1 >= graph_sizes)

#     # 3. è¿”å›æœ€ç»ˆç»“æœ
#     total_log_prob = log_probs_per_graph.sum()
    
#     # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥è¿”å›æŸå¤±ï¼Œå› ä¸ºæŸå¤±çš„è®¡ç®—éœ€è¦ rewardï¼Œè€Œ reward æ¥è‡ªäºç”Ÿæˆç½‘ç»œ
#     # æ‰€ä»¥æˆ‘ä»¬è¿”å›è®¡ç®—æŸå¤±æ‰€éœ€çš„ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šorders å’Œ total_log_prob
#     return orders, total_log_prob

    
def seg(orders, batch_fc):  # å¾…å®šä¹‰
    clean_batch = clean_batch
    return clean_batch

# ==============================================================================
# 4. ä¸»è®­ç»ƒå‡½æ•° (Main Training Function)
# ==============================================================================

def train(
    args,
    logger,
    train_loader, # è®­ç»ƒæ•°æ®
    val_loader, # æµ‹è¯•æ•°æ®
    s_model, # æ’åºç½‘ç»œå®ä¾‹
    model, # E_DiT_Network å®ä¾‹
    scheduler, # HierarchicalDiffusionScheduler å®ä¾‹
    fragmenter,
    amp_autocast,
    loss_scaler,
):
    """
    ä¸»è®­ç»ƒå‡½æ•°ã€‚
    """
    device = args.device
    model.to(device)
    s_model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_model = optim.Adam(model.parameters(), lr=args.learning_rate)
    optimizer_s_model = optim.Adam(s_model.parameters(), lr=args.s_learning_rate) # s_modelæœ‰ç‹¬ç«‹çš„å­¦ä¹ ç‡

    # # ç›‘æ§éªŒè¯æŸå¤±ï¼Œå¦‚æœè¿ç»­5æ¬¡éªŒè¯éƒ½æ²¡æœ‰æ”¹å–„ï¼Œåˆ™å°†å­¦ä¹ ç‡ä¹˜ä»¥0.5 (å‡åŠ)
    # # verbose=True ä¼šåœ¨å­¦ä¹ ç‡æ›´æ–°æ—¶æ‰“å°ä¸€æ¡æ¶ˆæ¯
    # scheduler_model = ReduceLROnPlateau(
    #     optimizer_model, mode='min', factor=0.5, patience=5, verbose=True
    # )
    # scheduler_s_model = ReduceLROnPlateau(
    #     optimizer_s_model, mode='min', factor=0.5, patience=5, verbose=True
    # )
    # # ---------------------------------------------
    # åˆ›å»º CosineAnnealingLR è°ƒåº¦å™¨
    T_max = args.epochs  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸ºæ€» epoch æ•°
    lr_min_factor = 0.01
    scheduler_model = CosineAnnealingLR(optimizer_model, T_max=T_max, eta_min=lr_min_factor * args.learning_rate)
    scheduler_s_model = CosineAnnealingLR(optimizer_s_model, T_max=T_max, eta_min=lr_min_factor * args.s_learning_rate)

    best_val_loss = float('inf')
    best_epoch = 0
    checkpoint_dir = os.path.join(args.output_dir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)

    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨: {checkpoint_dir}")
    logger.info("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(1, args.epochs + 1):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºâ€œè®­ç»ƒæ¨¡å¼â€
        # å®ƒä¼šé€šçŸ¥æ¨¡å‹ä¸­æ‰€æœ‰å…·æœ‰ä¸åŒè®­ç»ƒ/è¯„ä¼°è¡Œä¸ºçš„å±‚ï¼ˆä¸»è¦æ˜¯ Dropout å±‚å’Œ BatchNorm å±‚ï¼‰åˆ‡æ¢åˆ°å®ƒä»¬çš„è®­ç»ƒçŠ¶æ€ã€‚
        # Dropout å±‚åœ¨è®­ç»ƒæ—¶ä¼šéšæœºâ€œä¸¢å¼ƒâ€ä¸€äº›ç¥ç»å…ƒï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¸ä¼šä¸¢å¼ƒï¼Œä¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒã€‚
        # BatchNorm å±‚åœ¨è®­ç»ƒæ—¶ä¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶æ›´æ–°å…¶å†…éƒ¨çš„å…¨å±€ç»Ÿè®¡é‡ï¼›åœ¨è¯„ä¼°æ—¶åˆ™ä¼šä½¿ç”¨å·²å­¦ä¹ åˆ°çš„å…¨å±€ç»Ÿè®¡é‡ã€‚
        model.train()  
        s_model.train()

        total_loss_epoch = 0.0
        total_s_loss_epoch = 0.0
        
        # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
        pbar_train = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        
        for step, (batch_non_fc, batch_fc) in enumerate(pbar_train):
            batch_fc = batch_fc.to(device)
            batch_non_fc = batch_non_fc.to(device) # å°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¼ é‡ï¼Œå¦‚ pos, x, edge_index ç­‰ï¼‰ä¸€æ¬¡æ€§åœ°ç§»åŠ¨åˆ°ä¹‹å‰å®šä¹‰å¥½çš„ç›®æ ‡è®¾å¤‡ device ä¸Š
             
            # åœ¨æ¯ä¸ª step å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            optimizer_model.zero_grad()
            optimizer_s_model.zero_grad()

            # --- ä½¿ç”¨ amp_autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ ---
            # å°†æ‰€æœ‰çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—åŒ…è£¹åœ¨å†…
            with amp_autocast():

                # å°†éå…¨è¿æ¥çš„ batch é€å…¥æ’åºç½‘ç»œï¼Œè¿”å›æ’åºå’ŒæŸå¤±
                orders, log_prob_orders = s_model(batch_non_fc)

                # å¾—åˆ°åˆ‡åˆ†åçš„å­å›¾ç‰‡æ®µ
                clean_batch = seg(orders, batch_fc)

                # --- 0. å‡†å¤‡å·¥ä½œ ---
                num_graphs = clean_batch.num_graphs # æ‰¹æ¬¡ä¸­åŒ…å«çš„ç‹¬ç«‹å›¾çš„æ•°é‡ï¼ˆç­‰äº batch_sizeï¼‰ã€‚ç”¨äºé‡‡æ ·å›¾çº§åˆ«çš„å˜é‡ï¼Œå¦‚æ—¶é—´æ­¥ t
                num_nodes = clean_batch.num_nodes #  æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„èŠ‚ç‚¹æ€»æ•°
                num_edges = clean_batch.num_edges # æ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾çš„è¾¹æ€»æ•°

                # a. åæ ‡ç¼©æ”¾
                scaled_pos = scale_to_unit_sphere(clean_batch.pos, clean_batch.batch)
            
                # b. é‡‡æ ·æ—¶é—´æ­¥å’Œé«˜æ–¯å™ªå£°
                # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªå›¾éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ t1
                # t1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [batch_size] çš„å¼ é‡ï¼Œä¾‹å¦‚ tensor([18, 98, 21, ...])
                t1 = torch.randint(1, scheduler.T1 + 1, (num_graphs,), device=device) 
                t2 = torch.randint(1, scheduler.T2 + 1, (num_graphs,), device=device)
                # noise1 æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, 3] çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ã€‚noise1[i] å°±æ˜¯è¦åŠ åˆ°ç¬¬ i ä¸ªåŸå­åæ ‡ä¸Šçš„å™ªå£°å‘é‡ã€‚
                noise1 = torch.randn_like(scaled_pos)
                noise2 = torch.randn_like(scaled_pos)
            
                # c. å°† per-graph çš„æ—¶é—´æ­¥æ‰©å±•åˆ° per-node å’Œ per-edge
                # t1: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_graphs] çš„å¼ é‡ã€‚å‡è®¾ batch_size=4ï¼Œt1å¯èƒ½é•¿è¿™æ ·ï¼štensor([18, 98, 21, 76])
                # clean_batch.batch: ä¸€ä¸ªå½¢çŠ¶ä¸º [num_nodes] çš„å¼ é‡ï¼Œè®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªä¸ªå›¾ã€‚
                # å®ƒå¯èƒ½é•¿è¿™æ ·ï¼ˆå‡è®¾4ä¸ªå›¾åˆ†åˆ«æœ‰3, 2, 4, 3ä¸ªèŠ‚ç‚¹ï¼‰ï¼štensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3])ã€‚
                t1_per_node = t1[clean_batch.batch] # å½¢çŠ¶ä¸º[num_nodes]
                t1_per_edge = t1[clean_batch.batch[clean_batch.edge_index[0]]] # å½¢çŠ¶ä¸º[num_edges]


                # --- ç­–ç•¥ I: å…¨å±€å»å™ª (ç”Ÿæˆå™ªå£°å›¾ â… ) ---
                
                # a. åŠ å™ªåæ ‡
                noised_pos_I = scheduler.q_sample(scaled_pos, t1_per_node, noise1, schedule_type='alpha')  
            
                # b. åŠ å™ªåŸå­ç±»å‹
                noised_x_I = noise_discrete_features(clean_batch.x, scheduler.Q_bar_alpha_a, t1_per_node)
            
                # c. åŠ å™ªè¾¹å±æ€§
                noised_edge_attr_I = noise_discrete_features(clean_batch.edge_attr, scheduler.Q_bar_alpha_b, t1_per_edge)
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â… 
                # å¤åˆ¶å¹²å‡€çš„æ•°æ®ï¼Œæ›´æ”¹åŠ å™ªçš„éƒ¨åˆ†
                noised_data_I = clean_batch.clone()
                noised_data_I.pos = noised_pos_I
                noised_data_I.x = noised_x_I
                noised_data_I.edge_attr = noised_edge_attr_I
                
                # e. å‡†å¤‡æ¨¡å‹è¾“å…¥
                # åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºå½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰åŸå­çš„æ€»æ•°ï¼Œå†…å®¹å…¨ä¸º True çš„å‘é‡ã€‚
                target_node_mask_I = torch.ones(num_nodes, dtype=torch.bool, device=device)
                # å¤„ç†å¹¶è¾“å‡ºæ‰€æœ‰è¾¹çš„é¢„æµ‹ç»“æœ
                edge_index_to_predict_I = clean_batch.edge_index
                
                # f. æ¨¡å‹å‰å‘ä¼ æ’­
                predictions_I = model(noised_data_I, t1, target_node_mask_I, edge_index_to_predict_I)
            
                # g. è®¡ç®—æŸå¤± â… 
                lossI_a = calculate_atom_type_loss(
                    predictions_I['atom_type_logits'],
                    clean_batch.x.argmax(dim=-1),  # ä» One-Hot ç¼–ç çš„ç‰¹å¾å¼ é‡ä¸­ï¼Œæå–å‡ºæ¯ä¸ªé¡¹ç›®å¯¹åº”çš„ç±»åˆ«ç´¢å¼• (class index)
                    t1_per_node,
                    args.lambda_aux
                )
                lossI_r = calculate_coordinate_loss_wrapper(
                    predicted_r0=predictions_I['predicted_r0'], 
                    true_noise=noise1, 
                    r_t=noised_pos_I, 
                    t=t1_per_node, 
                    scheduler=scheduler, 
                    schedule_type='alpha'
                )
                lossI_b = calculate_bond_type_loss(
                    pred_logits=predictions_I['bond_logits'], 
                    true_b0_indices=clean_batch.edge_attr.argmax(dim=-1), 
                    t=t1_per_edge,
                    lambda_aux=args.lambda_aux
                )
                loss_I = args.w_a * lossI_a + args.w_r * lossI_r + args.w_b * lossI_b


                # --- ç­–ç•¥ II: å±€éƒ¨ç”Ÿæˆ (ç”Ÿæˆå™ªå£°å›¾ â…¡) ---

                # a. è¯†åˆ«ä¸Šä¸‹æ–‡å’Œç›®æ ‡
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ç›®æ ‡
                target_node_mask_II = clean_batch.is_last.squeeze() # is_last å°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡maskï¼Œç»´åº¦å‹ç¼©ä¸º[num_nodes]
                # æ ‡è¯†å“ªäº›èŠ‚ç‚¹æ˜¯ä¸Šä¸‹æ–‡èŠ‚ç‚¹ï¼Œç”¨äºå¯¹ä¸Šä¸‹æ–‡èŠ‚ç‚¹åŠ å™ª
                context_node_mask_II = ~target_node_mask_II
                # æ ‡è¯†å“ªäº›è¾¹æ˜¯ä¸é¢„æµ‹ç›®æ ‡èŠ‚ç‚¹ç›¸å…³çš„è¾¹
                # å¯¹äºç¬¬ i æ¡è¾¹ï¼Œå¦‚æœå®ƒçš„èµ·ç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹æˆ–è€…å®ƒçš„ç»ˆç‚¹æ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯éœ€è¦è¢«é¢„æµ‹çš„è¾¹
                target_edge_mask = (target_node_mask_II[clean_batch.edge_index[0]] | target_node_mask_II[clean_batch.edge_index[1]])
                # ç”¨äºå¯¹ä¸Šä¸‹æ–‡è¾¹åŠ å™ª
                context_edge_mask = ~target_edge_mask

                # b. å‡†å¤‡æ—¶é—´æ­¥ (T1 å’Œ t2)
                # åˆ›å»ºä¸€ä¸ªä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒã€ç±»å‹ç›¸åŒã€è®¾å¤‡ç›¸åŒçš„æ–°å¼ é‡ï¼Œå¹¶å°†æ‰€æœ‰å…ƒç´ å¡«å……ä¸ºT1
                t_T1_per_node = torch.full_like(t1_per_node, fill_value=scheduler.T1)
                t_T1_per_edge = torch.full_like(t1_per_edge, fill_value=scheduler.T1)
                t2_per_node = t2[clean_batch.batch]
                t2_per_edge = t2[clean_batch.batch[clean_batch.edge_index[0]]]

                # c. å¯¹ä¸Šä¸‹æ–‡å’Œç›®æ ‡åˆ†åˆ«åŠ å™ª
                # åæ ‡
                # è®¡ç®—å‡ºæ‰€æœ‰ä¸Šä¸‹æ–‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_context = scheduler.q_sample(scaled_pos[context_node_mask_II], t_T1_per_node[context_node_mask_II], noise2[context_node_mask_II], 'alpha')
                # è®¡ç®—å‡ºæ‰€æœ‰ç›®æ ‡åŸå­çš„åŠ å™ªååæ ‡
                noised_pos_target = scheduler.q_sample(scaled_pos[target_node_mask_II], t2_per_node[target_node_mask_II], noise2[target_node_mask_II], 'delta')
                # åˆ›å»ºä¸€ä¸ªç©ºçš„â€œç”»å¸ƒâ€
                noised_pos_II = torch.zeros_like(scaled_pos)
                # å°†è®¡ç®—å¥½çš„ä¸Šä¸‹æ–‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[context_node_mask_II] = noised_pos_context
                # å°†è®¡ç®—å¥½çš„ç›®æ ‡åæ ‡å¡«å……åˆ°ç”»å¸ƒçš„ç›¸åº”ä½ç½®
                noised_pos_II[target_node_mask_II] = noised_pos_target

                # åŸå­ç±»å‹
                noised_x_context = noise_discrete_features(clean_batch.x[context_node_mask_II], scheduler.Q_bar_alpha_a, t_T1_per_node[context_node_mask_II])
                noised_x_target = noise_discrete_features(clean_batch.x[target_node_mask_II], scheduler.Q_bar_gamma_a, t2_per_node[target_node_mask_II])
                noised_x_II = torch.zeros_like(clean_batch.x)
                noised_x_II[context_node_mask_II] = noised_x_context
                noised_x_II[target_node_mask_II] = noised_x_target
            
                # è¾¹å±æ€§
                noised_edge_attr_context = noise_discrete_features(clean_batch.edge_attr[context_edge_mask], scheduler.Q_bar_alpha_b, t_T1_per_edge[context_edge_mask])
                noised_edge_attr_target = noise_discrete_features(clean_batch.edge_attr[target_edge_mask], scheduler.Q_bar_gamma_b, t2_per_edge[target_edge_mask])
                noised_edge_attr_II = torch.zeros_like(clean_batch.edge_attr)
                noised_edge_attr_II[context_edge_mask] = noised_edge_attr_context
                noised_edge_attr_II[target_edge_mask] = noised_edge_attr_target
            
                # d. æ„å»ºåŠ å™ªåçš„æ•°æ®å¯¹è±¡ â…¡
                noised_data_II = clean_batch.clone()
                noised_data_II.pos = noised_pos_II
                noised_data_II.x = noised_x_II
                noised_data_II.edge_attr = noised_edge_attr_II
            
                # e. å‡†å¤‡æ¨¡å‹è¾“å…¥
                # target_node_mask_II å·²ç»å®šä¹‰å¥½äº†
                edge_index_to_predict_II = clean_batch.edge_index[:, target_edge_mask]
            
                # f. æ¨¡å‹å‰å‘ä¼ æ’­ (æ³¨æ„æ—¶é—´æ­¥ä¼ å…¥çš„æ˜¯ t2)
                predictions_II = model(noised_data_II, t2, target_node_mask_II, edge_index_to_predict_II)

                # g. è®¡ç®—æŸå¤± â…¡
                # æ³¨æ„ï¼šè¿™é‡Œçš„çœŸå®æ ‡ç­¾å’Œå™ªå£°éƒ½éœ€è¦æ ¹æ® mask è¿›è¡Œç­›é€‰
                lossII_a = calculate_atom_type_loss(
                    predictions_II['atom_type_logits'],
                    clean_batch.x[target_node_mask_II].argmax(dim=-1),
                    t2_per_node[target_node_mask_II],
                    args.lambda_aux
                )
                lossII_r = calculate_coordinate_loss_wrapper(
                    predicted_r0=predictions_II['predicted_r0'], 
                    true_noise=noise2[target_node_mask_II], 
                    r_t=noised_pos_target, 
                    t=t2_per_node[target_node_mask_II], 
                    scheduler=scheduler, 
                    schedule_type='delta'
                )
                lossII_b = calculate_bond_type_loss(
                    pred_logits=predictions_II['bond_logits'],
                    true_b0_indices=clean_batch.edge_attr[target_edge_mask].argmax(dim=-1),
                    t=t2_per_edge[target_edge_mask],
                    lambda_aux=args.lambda_aux
                )
                loss_II = args.w_a * lossII_a + args.w_r * lossII_r + args.w_b * lossII_b


                # --- æ€»æŸå¤±ä¸åå‘ä¼ æ’­ ---
                total_loss = scheduler.T1 * loss_I + scheduler.T2 * loss_II

                 # æ ¹æ® OM è®ºæ–‡, å¥–åŠ± R(Ï€) = log p_Î¸(G|Ï€) - log q_Ï†(Ï€|G)
                # log p_Î¸(G|Ï€) ç”±ç”Ÿæˆæ¨¡å‹çš„è´ŸæŸå¤± -total_loss è¿‘ä¼¼
                # log q_Ï†(Ï€|G) æ˜¯æ’åºç½‘ç»œè¾“å‡ºçš„ log_prob_orders
                # æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ– E[R(Ï€)], æ‰€ä»¥å°† R(Ï€) ä½œä¸º REINFORCE çš„å¥–åŠ±
                reward = (-total_loss - log_prob_orders).detach()  # å¥–åŠ±å¿…é¡»ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å¸¦æ¢¯åº¦

                # ç­–ç•¥æŸå¤± L_policy = -R(Ï€) * log q_Ï†(Ï€|G)
                loss_s_model = -reward * log_prob_orders

            # --- 4. åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–° ---
            # ä½¿ç”¨ loss_scaler åˆ†åˆ«ç¼©æ”¾ä¸¤ä¸ªæ¨¡å‹çš„æŸå¤±
            # backward() ä¼šç´¯åŠ æ¢¯åº¦ï¼Œè€Œæˆ‘ä»¬å·²ç»åœ¨ step å¼€å§‹æ—¶æ¸…é›¶äº†æ¢¯åº¦
            
            # ç¼©æ”¾ç”Ÿæˆæ¨¡å‹çš„æŸå¤±å¹¶è®¡ç®—æ¢¯åº¦
            # retain_graph=True æ˜¯å› ä¸º s_model çš„æŸå¤±è®¡ç®—ä¹Ÿä¾èµ–äºè¿™ä¸ªå›¾
            loss_scaler.scale(total_loss).backward(retain_graph=True)
            
            # ç¼©æ”¾æ’åºç½‘ç»œçš„æŸå¤±å¹¶è®¡ç®—æ¢¯åº¦
            loss_scaler.scale(loss_s_model).backward()

            # æ¢¯åº¦åç¼©æ”¾ï¼ˆunscalingï¼‰ å’Œ æ¨¡å‹å‚æ•°æ›´æ–°
            # loss_scaler.step ä¼šè‡ªåŠ¨ unscale æ¢¯åº¦ï¼Œç„¶åè°ƒç”¨ä¼˜åŒ–å™¨çš„ step
            loss_scaler.step(optimizer_model)
            loss_scaler.step(optimizer_s_model)
            
            # æ›´æ–° scaler çš„ç¼©æ”¾å› å­
            loss_scaler.update()

            
            total_loss_epoch += total_loss.item()
            total_s_loss_epoch += loss_s_model.item()
            pbar_train.set_postfix({
                'loss_G': f"{total_loss.item():.2f}", 
                'loss_S': f"{loss_s_model.item():.2f}"
            })
            
        avg_train_loss = total_loss_epoch / len(train_loader)
        avg_s_train_loss = total_s_loss_epoch / len(train_loader)
        logger.info(f"Epoch {epoch} [Train] å®Œæˆ, å¹³å‡ç”ŸæˆæŸå¤±: {avg_train_loss:.4f}, å¹³å‡æ’åºæŸå¤±: {avg_s_train_loss:.4f}")


        # --- éªŒè¯é˜¶æ®µ ---
        if epoch >= 10 and (epoch % 5 == 0):
            avg_val_loss, avg_s_val_loss = validate(val_loader, model, s_model, scheduler, args, amp_autocast)
            logger.info(f"Epoch {epoch} [Validation] å®Œæˆ, å¹³å‡æŸå¤±: {avg_val_loss:.4f}")

            # # --- ä½¿ç”¨éªŒè¯æŸå¤±æ¥æ›´æ–°å­¦ä¹ ç‡ ---
            # # scheduler.step() ä¼šç›‘æ§ avg_val_lossï¼Œå¹¶æ ¹æ®å†…éƒ¨çš„ patience è®¡æ•°å†³å®šæ˜¯å¦é™ä½ LR
            # scheduler_model.step(avg_val_loss)
            # scheduler_s_model.step(avg_s_val_loss)
            # # -----------------------------------------


            # ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹ 
            logger.info(f"åœ¨ Epoch {epoch} ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹åŠå…¶éªŒè¯æŸå¤±...")

            checkpoint_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                's_model_state_dict': s_model.state_dict(),
                'optimizer_model_state_dict': optimizer_model.state_dict(),
                'optimizer_s_model_state_dict': optimizer_s_model.state_dict(),
                'scheduler_model_state_dict': scheduler_model.state_dict(),    
                'scheduler_s_model_state_dict': scheduler_s_model.state_dict(), 
                'loss_scaler_state_dict': loss_scaler.state_dict(),
                'validation_loss': avg_val_loss, 
                's_validation_loss': avg_s_val_loss,
                'args': args
            }
            # ä½¿ç”¨åŒ…å« epoch ç¼–å·çš„å”¯ä¸€æ–‡ä»¶å
            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint_state, checkpoint_path)

            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_epoch = epoch
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜æœ€ä½³æ¨¡å‹...")
                
                # ä¸ºæœ€ä½³æ¨¡å‹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ä¿å­˜çŠ¶æ€
                best_model_state = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    's_model_state_dict': s_model.state_dict(),
                    'best_val_loss': best_val_loss,
                    'args': args
                }
                
                best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
                torch.save(best_model_state, best_model_path)

            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_model.step()
            scheduler_s_model.step()

    logger.info("è®­ç»ƒå®Œæˆã€‚")
    logger.info(f"æœ€ç»ˆï¼Œæœ€ä½³æ¨¡å‹å‘ç°åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º: {best_val_loss:.4f}")
