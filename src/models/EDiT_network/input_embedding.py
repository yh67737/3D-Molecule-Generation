import torch
import torch.nn as nn
import math
from torch_scatter import scatter

from e3nn import o3

from .radial_func import RadialProfile
from .tensor_product_rescale import LinearRS, FullyConnectedTensorProductRescale, sort_irreps_even_first, TensorProductRescale

_RESCALE = True
_USE_BIAS = True

def _debug_print_irreps_norm(tensor_name: str, tensor: torch.Tensor, irreps_blueprint: o3.Irreps):
    """ä¸€ä¸ªç‹¬ç«‹çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰“å°å¼ é‡çš„åˆ†è§£èŒƒæ•°ã€‚"""
    print(f"\n--- [Debug] æ­£åœ¨åˆ†æ '{tensor_name}' (å½¢çŠ¶: {tensor.shape}) ---")
    print(f"  - ä½¿ç”¨è“å›¾: {irreps_blueprint}")
    
    if tensor.shape[-1] != irreps_blueprint.dim:
        print(f"  - ğŸ”´ ä¸¥é‡é”™è¯¯: å¼ é‡ç»´åº¦ ({tensor.shape[-1]}) ä¸è“å›¾ç»´åº¦ ({irreps_blueprint.dim}) ä¸åŒ¹é…ï¼")
        return

    print("  - åˆ†è§£èŒƒæ•°:")
    current_start_dim = 0
    for mul, ir in irreps_blueprint:
        part_dim = mul * ir.dim
        start_index = current_start_dim
        
        feature_part = tensor.narrow(1, start_index, part_dim)
        norm = torch.linalg.norm(feature_part).item()
        
        log_str = f"    - Irrep '{mul}{ir}': "
        log_str += f"ç»´åº¦èŒƒå›´ [{start_index}:{start_index + part_dim - 1}], "
        log_str += f"èŒƒæ•° = {norm:.6f}"
        print(log_str)
        
        if ir.l > 0 and norm < 1e-6:
            print(f"    - ğŸ”´ è­¦å‘Š: é«˜é˜¶éƒ¨åˆ† l={ir.l} çš„èŒƒæ•°æ¥è¿‘äºé›¶ï¼")
            
        current_start_dim += part_dim
    print("--- ç»“æŸåˆ†æ ---")

@torch.jit.script
def gaussian(x, mean, std):
    """é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°"""
    pi = 3.14159
    a = (2*pi) ** 0.5
    return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std)


# From Graphormer
class GaussianRadialBasisLayer(torch.nn.Module):
    def __init__(self, num_basis, cutoff):
        super().__init__()
        self.num_basis = num_basis
        self.cutoff = cutoff + 0.0  # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°

        self.mean = torch.nn.Parameter(torch.zeros(1, self.num_basis))
        self.std = torch.nn.Parameter(torch.zeros(1, self.num_basis))

        self.weight = torch.nn.Parameter(torch.ones(1, 1))
        self.bias = torch.nn.Parameter(torch.zeros(1, 1))

        self.std_init_max = 1.0
        self.std_init_min = 1.0 / self.num_basis
        self.mean_init_max = 1.0
        self.mean_init_min = 0

        torch.nn.init.uniform_(self.mean, self.mean_init_min, self.mean_init_max)
        torch.nn.init.uniform_(self.std, self.std_init_min, self.std_init_max)
        torch.nn.init.constant_(self.weight, 1)
        torch.nn.init.constant_(self.bias, 0)

    def forward(self, dist):
        x = dist / self.cutoff
        x = x.unsqueeze(-1)
        x = self.weight * x + self.bias
        x = x.expand(-1, self.num_basis)

        mean = self.mean
        std = self.std.abs() + 1e-5

        x = gaussian(x, mean, std)
        return x

    def extra_repr(self):
        return 'mean_init_max={}, mean_init_min={}, std_init_max={}, std_init_min={}'.format(
            self.mean_init_max, self.mean_init_min, self.std_init_max, self.std_init_min)


_MAX_ATOM_TYPE = 6 # 5ç§åŸå­ç±»å‹+1ç§å¸æ”¶æ€ 

class NodeEmbeddingNetwork(nn.Module):
    """
    ä¿®æ”¹è‡ªEquiformerçš„NodeEmbeddingNetworkï¼Œé¢å¤–å¤„ç†ç¯ä¿¡æ¯
    
    è¾“å…¥:
        - åŸå­ç±»å‹çš„one-hotç¼–ç 
        - ç¯ä¿¡æ¯çš„äºŒè¿›åˆ¶æ ‡å¿—
    è¾“å‡º:
        - åˆå§‹çº¯æ ‡é‡(L=0)çš„èŠ‚ç‚¹irrepsç‰¹å¾
    """
    
    def __init__(self, irreps_node_embedding: str = '128x0e+64x1e+32x2e', 
                 num_atom_types: int = _MAX_ATOM_TYPE, hidden_dim: int = 32, bias: bool = True):
        """
        Args:
            irreps_node_embedding (str): è¾“å‡ºçš„irrepså­—ç¬¦ä¸². 
            num_atom_types (int): åŸå­ç±»å‹æ•°é‡ (one-hotå‘é‡é•¿åº¦).
            hidden_dim (int): é¦–å…ˆå°†åŸå­ç±»å‹å’Œæ¡ä»¶æ ‡å¿—ä½æŠ•å°„åˆ°hidden_dimç›¸åŠ .
            bias (bool): çº¿æ€§å±‚æ˜¯å¦ä½¿ç”¨åç½®.
        """
        super().__init__()
        
        # ä¿å­˜è¾“å‡ºirrepsçš„å®šä¹‰
        self.irreps_node_embedding = o3.Irreps(irreps_node_embedding)
        
        # æˆç¯ä¿¡æ¯åµŒå…¥å±‚
        self.ring_embedding = nn.Embedding(num_embeddings=2, embedding_dim=hidden_dim) 
        # åŸå­ç±»å‹åµŒå…¥å±‚
        self.atom_type_embedding = nn.Linear(num_atom_types, hidden_dim, bias=False)
        
        # å®šä¹‰ç­‰å˜çº¿æ€§å±‚
        self.num_input_features = hidden_dim
        input_irreps = o3.Irreps(f'{self.num_input_features}x0e')
        self.feature_lin = LinearRS(input_irreps, self.irreps_node_embedding, bias=bias) 
        
        # çº¿æ€§å±‚æƒé‡åˆå§‹åŒ–
        self.feature_lin.tp.weight.data.mul_(self.num_input_features ** 0.5)
        
        
    def forward(self, atom_type_onehot: torch.Tensor, ring_info: torch.Tensor) -> tuple:
        """
        Args:
            atom_type_onehot (Tensor): one-hotç¼–ç çš„åŸå­ç±»å‹, shape [num_nodes, num_atom_types].
            ring_info (Tensor): ç¯ä¿¡æ¯, shape:[num_nodes,1]ï¼Œå–å€¼ä¸º0æˆ–1.

        Returns:
            node_embedding: èåˆåçš„åˆå§‹èŠ‚ç‚¹ç‰¹å¾.
        """
        # ç¡®ä¿ring_infoå½¢çŠ¶æ­£ç¡®[num_nodes, 1]
        if ring_info.ndim == 1:
            ring_info = ring_info.unsqueeze(-1)
            
        # å°†æˆç¯æ ‡å¿—(0/1)é€šè¿‡Embeddingå±‚æ˜ å°„
        ring_embeds = self.ring_embedding(ring_info.long().squeeze(-1))
        # å°†åŸå­ç±»å‹é€šè¿‡Embeddingå±‚æ˜ å°„
        atom_type_embeds = self.atom_type_embedding(atom_type_onehot.float())
        combined_attr = ring_embeds + atom_type_embeds

        node_embedding = self.feature_lin(combined_attr)
        
        # è¿”å›åµŒå…¥ç»“æœ
        return node_embedding


# å®šä¹‰åŒ–å­¦é”®ç±»å‹æ•°é‡
_DEFAULT_NUM_BOND_TYPES = 5 # æ— é”®ã€å•é”®ã€åŒé”®ã€ä¸‰é”®ã€èŠ³é¦™é”®


class EdgeEmbeddingNetwork(nn.Module):
    def __init__(self,
                 irreps_sh: str = '1x0e+1x1o+1x2e',
                 max_radius: float = 1000.0,
                 number_of_basis: int = 128,
                 num_bond_types: int = _DEFAULT_NUM_BOND_TYPES,
                 bond_embedding_dim: int = 32,
                 irreps_out_fused: str = '128x0e+64x1o+32x2e'):
        
        super().__init__()
        self.max_radius = max_radius
        self.irreps_sh = o3.Irreps(irreps_sh)
        self.irreps_out_fused = o3.Irreps(irreps_out_fused, )

        self.rbf = GaussianRadialBasisLayer(number_of_basis, cutoff=self.max_radius)
        self.bond_embedding = nn.Linear(num_bond_types, bond_embedding_dim, bias=False)

        scalar_dim_combined = number_of_basis + bond_embedding_dim
        irreps_scalar_combined = o3.Irreps(f'{scalar_dim_combined}x0e')

        self.fusion_tp = FullyConnectedTensorProductRescale(
            irreps_in1=self.irreps_sh,
            irreps_in2=irreps_scalar_combined,
            irreps_out=self.irreps_out_fused
        )

    def forward(self,
                pos: torch.Tensor,
                bond_type_onehot: torch.Tensor,
                edge_index: torch.Tensor) -> dict:
        edge_src, edge_dst = edge_index[0], edge_index[1]

        edge_vec = pos.index_select(0, edge_src) - pos.index_select(0, edge_dst)

        edge_attr = o3.spherical_harmonics(
            l=self.irreps_sh,
            x=edge_vec,
            normalize=True,
            normalization='component'
        )

        edge_length = edge_vec.norm(dim=1)
        edge_rbf = self.rbf(edge_length)

        bond_embeds = self.bond_embedding(bond_type_onehot.float())

        edge_scalars_combined = torch.cat([edge_rbf, bond_embeds], dim=-1)

        fused_edge_feature = self.fusion_tp(edge_attr, edge_scalars_combined)

        return {
            "fused_edge_feature": fused_edge_feature,
            "edge_attr_base": edge_attr,
            "edge_scalars_base": edge_rbf,
            "edge_src": edge_src,
            "edge_dst": edge_dst,
            "edge_vec": edge_vec
        }

class ScaledScatter(torch.nn.Module):
    def __init__(self, avg_aggregate_num):
        super().__init__()
        self.avg_aggregate_num = avg_aggregate_num + 0.0


    def forward(self, x, index, **kwargs):
        out = scatter(x, index, **kwargs)
        out = out.div(self.avg_aggregate_num ** 0.5)
        return out
    
    
    def extra_repr(self):
        return 'avg_aggregate_num={}'.format(self.avg_aggregate_num)
    
def DepthwiseTensorProduct(irreps_node_input, irreps_edge_attr, irreps_node_output, 
    internal_weights=False, bias=True):
    '''
        The irreps of output is pre-determined. 
        `irreps_node_output` is used to get certain types of vectors.
    '''
    irreps_output = []
    instructions = []
    
    for i, (mul, ir_in) in enumerate(irreps_node_input):
        for j, (_, ir_edge) in enumerate(irreps_edge_attr):
            for ir_out in ir_in * ir_edge:
                if ir_out in irreps_node_output or ir_out == o3.Irrep(0, 1):
                    k = len(irreps_output)
                    irreps_output.append((mul, ir_out))
                    instructions.append((i, j, k, 'uvu', True))
        
    irreps_output = o3.Irreps(irreps_output)
    irreps_output, p, _ = sort_irreps_even_first(irreps_output) #irreps_output.sort()
    instructions = [(i_1, i_2, p[i_out], mode, train)
        for i_1, i_2, i_out, mode, train in instructions]
    tp = TensorProductRescale(irreps_node_input, irreps_edge_attr,
            irreps_output, instructions,
            internal_weights=internal_weights,
            shared_weights=internal_weights,
            bias=bias, rescale=_RESCALE)
    return tp  

class EdgeDegreeEmbeddingNetwork(torch.nn.Module):
    def __init__(self, num_atom_types, hidden_dim, irreps_node_embedding, irreps_edge_attr, 
                 avg_aggregate_num, fc_neurons=[64, 64]): ### avg_aggregate_num
        super().__init__()    
        # æˆç¯ä¿¡æ¯åµŒå…¥å±‚
        self.ring_embedding = nn.Embedding(num_embeddings=2, embedding_dim=hidden_dim) 
        # åŸå­ç±»å‹åµŒå…¥å±‚
        self.atom_type_embedding = nn.Linear(num_atom_types, hidden_dim, bias=False)

        self.exp = LinearRS(o3.Irreps('1x0e'), irreps_node_embedding, 
            bias=_USE_BIAS, rescale=_RESCALE)
        self.dw = DepthwiseTensorProduct(o3.Irreps(f'{hidden_dim}x0e'), 
            irreps_edge_attr, irreps_node_embedding, 
            internal_weights=False, bias=False)
        self.rad = RadialProfile(fc_neurons + [self.dw.tp.weight_numel])
        for (slice, slice_sqrt_k) in self.dw.slices_sqrt_k.values():
            self.rad.net[-1].weight.data[slice, :] *= slice_sqrt_k
            self.rad.offset.data[slice] *= slice_sqrt_k
        self.proj = LinearRS(self.dw.irreps_out.simplify(), irreps_node_embedding)
        self.scale_scatter = ScaledScatter(avg_aggregate_num)
        
    
    def forward(self, atom_type_onehot: torch.Tensor, ring_info: torch.Tensor, edge_attr, edge_scalars, edge_src, edge_dst, batch):
        # ç¡®ä¿ring_infoå½¢çŠ¶æ­£ç¡®[num_nodes, 1]
        if ring_info.ndim == 1:
            ring_info = ring_info.unsqueeze(-1)
            
        # å°†æˆç¯æ ‡å¿—(0/1)é€šè¿‡Embeddingå±‚æ˜ å°„
        ring_embeds = self.ring_embedding(ring_info.long().squeeze(-1))
        # å°†åŸå­ç±»å‹é€šè¿‡Embeddingå±‚æ˜ å°„
        atom_type_embeds = self.atom_type_embedding(atom_type_onehot.float())
        node_features = ring_embeds + atom_type_embeds
        # if debug_irreps_blueprint is not None:
        #     # è°ƒç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„è¾…åŠ©å‡½æ•°
        #     _debug_print_irreps_norm(
        #         "node_features (after self.exp)",
        #         node_features,
        #         debug_irreps_blueprint
        #     )
        weight = self.rad(edge_scalars)
        edge_features = self.dw(node_features[edge_src], edge_attr, weight)
        edge_features = self.proj(edge_features)
        node_features = self.scale_scatter(edge_features, edge_dst, dim=0, 
            dim_size=node_features.shape[0])
        return node_features


class PositionalEncoding(nn.Module):
    """
    functionï¼šæ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç æ¨¡å—-->ç¼–ç ä»…ä¸d_modelå’Œmax_lenæœ‰å…³ï¼Œä¸è¯å‘é‡æ— å…³

    """
    def __init__(self, d_model: int, max_len: int = 500):
        '''
        Args:
            d_model (int): The dimension of the embedding. åµŒå…¥å‘é‡å¤§å°/èŠ‚ç‚¹ç‰¹å¾h_0çš„ç»´åº¦å¤§å°
            max_len (int): The maximum possible length of a sequence. åºåˆ—æœ€å¤§é•¿åº¦/æœ€å¤§åŸå­æ•°

        '''
        super(PositionalEncoding, self).__init__() # æ„é€ æ–¹æ³•åˆå§‹åŒ–

        ## åˆ›å»ºä½ç½®ç´¢å¼•å¼ é‡
        # positionå¼ é‡åŒ…å«æ‰€æœ‰å¯èƒ½çš„ä½ç½®pos(ä»0åˆ°max_len-1)
        # .unsqueeze(1)å°†positionå½¢çŠ¶ä» max_len]å˜ä¸º[max_len, 1]ï¼Œä¸ºåç»­å¹¿æ’­åšå‡†å¤‡
        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1) # [max_len, 1]

        ## è®¡ç®—é¢‘ç‡ï¼š1 / 10000^(2i/d_model)
        # torch.arange(0, d_model, 2)æå–æ‰€æœ‰å¶æ•°ç»´åº¦ç´¢å¼•[0, 2, 4, ...,](ä¸åŒ…å«d_model)
        # `exp(x * (-log(y)))` ç­‰ä»·äº `y^(-x)`
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model//2]

        ## åˆå§‹åŒ–ä½ç½®ç¼–ç çŸ©é˜µ
        # åˆ›å»ºä¸€ä¸ª[max_len, d_model]å¤§å°çš„é›¶çŸ©é˜µï¼Œç”¨äºå­˜æ”¾æœ€ç»ˆçš„ç¼–ç ç»“æœ
        pe = torch.zeros(max_len, d_model)

        ## è®¡ç®—å¹¶å¡«å……peå¶æ•°å’Œå¥‡æ•°ç»´åº¦
        # åˆ©ç”¨PyTorchçš„å¹¿æ’­æœºåˆ¶ï¼Œ[max_len, 1]çš„positionå’Œ[d_model/2]çš„div_termç›¸ä¹˜å¾—åˆ°[max_len, d_model//2]ï¼Œå…¶(pos, i)å…ƒç´ å€¼ä¸ºpos / 10000^(2i/d_model)
        # pe[:, 0::2]é€‰æ‹©æ‰€æœ‰å¶æ•°ç´¢å¼•åˆ—
        pe[:, 0::2] = torch.sin(position * div_term)
        # pe[:, 1::2]é€‰æ‹©æ‰€æœ‰å¥‡æ•°ç´¢å¼•åˆ—
        pe[:, 1::2] = torch.cos(position * div_term)

        ## å°†peæ³¨å†Œä¸ºæ¨¡å‹çš„buffer
        # register_bufferå°†peçŸ©é˜µä½œä¸ºæ¨¡å‹çŠ¶æ€çš„ä¸€éƒ¨åˆ†ä¿å­˜ä¸‹æ¥(state_dict)
        # ä½†å®ƒä¸æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œä¸ä¼šåœ¨åå‘ä¼ æ’­æ—¶è¢«æ›´æ–°æ¢¯åº¦
        # å¯¹äºå­˜å‚¨å›ºå®šçš„éè®­ç»ƒçš„æ•°æ®ï¼ˆå¦‚ä½ç½®ç¼–ç ï¼‰æ˜¯æ ‡å‡†åšæ³•ï¼›å¥½å¤„æ˜¯å½“è°ƒç”¨ model.to(device) æ—¶ï¼Œè¿™ä¸ªbufferä¼šè‡ªåŠ¨è¢«ç§»åŠ¨åˆ°ç›¸åº”è®¾å¤‡
        self.register_buffer('pe', pe) 

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        """
        Args:
            t (torch.Tensor): A tensor of shape [num_nodes] containing position indices.
        Returns:
            torch.Tensor: Positional encodings of shape [num_nodes, d_model].
        """
        # è¾“å…¥tåŒ…å«éœ€è¦æŸ¥æ‰¾çš„ä½ç½®ç´¢å¼•,å¦‚[0, 1, 2, 0](å¼ é‡)
        # self.peæ˜¯é¢„å…ˆè®¡ç®—å¥½çš„[max_len, d_model]å¤§å°çš„ç¼–ç è¡¨
        # PyTorchç´¢å¼•æœºåˆ¶å°†ä»peè¡¨ä¸­å–å‡ºç¬¬0, 1, 2, 0è¡Œï¼Œç»„æˆ[4, d_model]çš„è¾“å‡ºå¼ é‡è¿”å›
        return self.pe[t]


class InputEmbeddingLayer(nn.Module):
    """
    ä¸€ä¸ªå®Œæ•´çš„è¾“å…¥åµŒå…¥æ¨¡å—ã€‚
    å®ƒæ¥æ”¶ä¸€ä¸ªPyG Dataå¯¹è±¡ï¼Œè°ƒç”¨æ‰€æœ‰å­åµŒå…¥ç½‘ç»œï¼Œ
    å¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰E-DiT Blockæ‰€éœ€è¾“å…¥çš„å­—å…¸ã€‚
    """

    def __init__(self,
                 # NodeEmbeddingNetwork å‚æ•°
                 irreps_node_embedding: str,
                 num_atom_types: int,
                 node_embedding_hidden_dim: int,
                 # EdgeEmbeddingNetwork å‚æ•°
                 irreps_sh: str,
                 max_radius: float,
                 num_rbf: int,
                 num_bond_types: int,
                 bond_embedding_dim: int,
                 irreps_edge_fused: str,
                 # EdgeDegreeEmbeddingNetwork å‚æ•°
                 avg_degree: float,
                 # PositionalEncodingå‚æ•°-->æœ€å¤§åŸå­æ•°
                 max_seq_len: int = 100):
        super().__init__()

        # Irreps å®šä¹‰
        self.irreps_node_embedding = o3.Irreps(irreps_node_embedding)
        self.irreps_sh = o3.Irreps(irreps_sh)
        self.irreps_edge_fused = o3.Irreps(irreps_edge_fused)

        # å®ä¾‹åŒ–ä¸‰ä¸ªæ ¸å¿ƒå­æ¨¡å—
        self.node_embedding_net = NodeEmbeddingNetwork(
            irreps_node_embedding=self.irreps_node_embedding,
            num_atom_types=num_atom_types,
            hidden_dim=node_embedding_hidden_dim
        )
        self.edge_embedding_net = EdgeEmbeddingNetwork(
            irreps_sh=self.irreps_sh,
            max_radius=max_radius,
            number_of_basis=num_rbf,
            num_bond_types=num_bond_types,
            bond_embedding_dim=bond_embedding_dim,
            irreps_out_fused=self.irreps_edge_fused
        )
        self.edge_degree_net = EdgeDegreeEmbeddingNetwork(
            num_atom_types=num_atom_types,
            hidden_dim=node_embedding_hidden_dim,
            irreps_node_embedding=self.irreps_node_embedding,
            irreps_edge_attr=self.irreps_sh,
            fc_neurons=[num_rbf, num_rbf],
            avg_aggregate_num=avg_degree
        )

        # å®ä¾‹åŒ–ä½ç½®ç¼–ç æ¨¡å—
        # è‡ªåŠ¨ä»irrepsä¸­æå–èŠ‚ç‚¹ç¼–ç æ ‡é‡éƒ¨åˆ†çš„ç»´åº¦
        self.scalar_dim = sum(mul for mul, ir in self.irreps_node_embedding if ir.l == 0)
        if self.scalar_dim == 0:
            raise ValueError("irreps_node_embedding must contain a scalar (0e) component for positional encoding.")
        
        # å®ä¾‹åŒ–PositionalEncoding
        self.positional_encoding = PositionalEncoding(d_model=self.scalar_dim, max_len=max_seq_len)

    def forward(self, data) -> dict:
        """
        æ¥æ”¶ä¸€ä¸ªPyG Data/Batchå¯¹è±¡å¹¶è¿›è¡Œå®Œæ•´çš„åµŒå…¥æ“ä½œã€‚

        Args:
            data: ä¸€ä¸ªåŒ…å«x, pos, edge_index, edge_attr, pring_outç­‰å±æ€§çš„Dataå¯¹è±¡ã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«æ‰€æœ‰E-DiT Blockæ‰€éœ€è¾“å…¥çš„å­—å…¸ã€‚
        """
        # å¤„ç†è¾¹ç‰¹å¾
        edge_info = self.edge_embedding_net(
            pos=data.pos,
            bond_type_onehot=data.edge_attr,
            edge_index=data.edge_index
        )

        # å¤„ç†èŠ‚ç‚¹åˆå§‹ç‰¹å¾
        initial_node_embedding = self.node_embedding_net(
            atom_type_onehot=data.x,
            ring_info=data.pring_out
        )

        # ä¸ºèŠ‚ç‚¹æ³¨å…¥å‡ ä½•ä¿¡æ¯
        edge_degree_supplement = self.edge_degree_net(
            atom_type_onehot=data.x,
            ring_info=data.pring_out,
            edge_attr=edge_info['edge_attr_base'],
            edge_scalars=edge_info['edge_scalars_base'],
            edge_src=edge_info['edge_src'],
            edge_dst=edge_info['edge_dst'],
            batch=data.batch
        )

        # æœ€ç»ˆèŠ‚ç‚¹ç‰¹å¾èåˆ
        final_node_features = edge_degree_supplement

        # ä¸ºèŠ‚ç‚¹ç‰¹å¾æ·»åŠ ä½ç½®ç¼–ç 
        # ä¸ºæ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªå›¾ç”Ÿæˆä»0å¼€å§‹çš„åŸå­ç´¢å¼•
        if data.batch is not None:
            # è¿™æ˜¯æ‰¹å¤„ç†çš„æƒ…å†µ (ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒæœŸé—´)
            num_atoms_per_graph = torch.bincount(data.batch)
        else:
            # è¿™æ˜¯å•ä¸ªå›¾çš„æƒ…å†µ (ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆ/æ¨ç†æœŸé—´)
            # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªå¼ é‡ï¼Œä½¿å…¶æ ¼å¼ä¸ bincount çš„è¾“å‡ºä¸€è‡´
            # å¹¶ä¸”ç¡®ä¿å®ƒåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            device = data.x.device if hasattr(data, 'x') and data.x is not None else 'cpu' # ç¡®ä¿è®¾å¤‡æ­£ç¡®
            num_atoms_per_graph = torch.tensor([data.num_nodes], device=device)
        atom_indices = torch.cat([torch.arange(n) for n in num_atoms_per_graph]).to(data.pos.device)

        # è·å–ä½ç½®ç¼–ç 
        pos_enc = self.positional_encoding(atom_indices)

        # å°†ä½ç½®ç¼–ç ä»…åŠ åˆ°ç‰¹å¾çš„æ ‡é‡éƒ¨åˆ†
        scalar_features = final_node_features.narrow(1, 0, self.scalar_dim)
        higher_order_features = final_node_features.narrow(1, self.scalar_dim, final_node_features.shape[1] - self.scalar_dim)
        
        scalar_features_with_pe = scalar_features + pos_enc
        
        final_node_features_with_pe = torch.cat([scalar_features_with_pe, higher_order_features], dim=1)

        # å‡†å¤‡E-DiT Blockçš„å…¨éƒ¨è¾“å…¥
        node_attr = data.x
    
        return {
            "node_input": final_node_features_with_pe, 
            "node_attr": node_attr,
            "edge_src": edge_info['edge_src'],
            "edge_dst": edge_info['edge_dst'],
            "edge_index": data.edge_index,
            "edge_input": edge_info['fused_edge_feature'],
            "edge_attr_type": data.edge_attr,
            "batch": data.batch
        }